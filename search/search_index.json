{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome Page","text":"<p>Welcome to the JH How To Guide! On this website you can find a variety of tips and tricks to help you be more efficient with the computers you are using.</p> <p>The general idea behind this guide is to have a personal wiki which you can consult and edit to your hearts content! Whether you found an answer after a long search on the web, or want a simple reminder of a terminal command, this guide is here to help!</p>"},{"location":"FFMPEG%20%26%20ImageMagick/animate_images/","title":"Animate Images","text":""},{"location":"FFMPEG%20%26%20ImageMagick/animate_images/#create-gif-from-images","title":"Create GIF from Images","text":"<p>Open the terminal and go to the directory where the images are located. For this example, let us say that we have a list of images called frame001.png, frame002.png, etc.</p> <p>Type in the command: <pre><code>ffmpeg -i frame%03d.png -vf fps=10,scale=720:-1 output.gif\n</code></pre></p> <p>And you should have a gif named <code>output.gif</code>.</p>"},{"location":"FFMPEG%20%26%20ImageMagick/animate_images/#create-video-from-images","title":"Create Video from Images","text":"<p>Open the terminal and go to the directory where the images are located. For this example, let us say that we have a list of images called frame001.png, frame002.png, etc.</p> <p>Type in the command: <pre><code>ffmpeg -framerate 30 -pattern_type glob -i '*.png' -c:v libx264 -pix_fmt yuv420p out.mp4\n</code></pre></p>"},{"location":"FFMPEG%20%26%20ImageMagick/change_resolution/","title":"Change Resolution","text":""},{"location":"FFMPEG%20%26%20ImageMagick/change_resolution/#change-image-resolution","title":"Change Image Resolution","text":"<p>You can use ImageMagick's <code>convert</code> function to change the resolution of an image: <pre><code>convert original.png -resize 100x100 new.png\n</code></pre></p> <p>To resize an image to specific dimensions, without maintaining the original image\u2019s aspect ratio, include a bang <code>!</code> at the end of the dimensions: <pre><code>convert original.png -resize 100x100! new.png\n</code></pre></p> <p>You can also use percentages with the resize flag. For example, if you want to make an image smaller by 50%, all you need to type is: <pre><code>magick original.png -resize 50% new.png\n</code></pre></p>"},{"location":"FFMPEG%20%26%20ImageMagick/change_resolution/#change-video-resolution","title":"Change Video Resolution","text":"<p>If you want to change the resolution of a video to 480x320, you can do this simply by entering the command: <pre><code>ffmpeg -i input.mp4 -vf \"scale=480:320\" output_320.mp4\n</code></pre></p> <p>Alternatively, if you know the width you want and you want to keep the same aspect ratio, you can use <code>-1</code>: <pre><code>ffmpeg -i input.mp4 -vf \"scale=480:-1\" output_320.mp4\n</code></pre></p> <p>If you want to change the aspect ratio, you can use the <code>setdar</code> flag: <pre><code>ffmpeg -i input.mp4 -vf \"scale=480:320,setdar=4:3\" output_320.mp4\n</code></pre></p>"},{"location":"FFMPEG%20%26%20ImageMagick/convert_many_images/","title":"Convert Many Image Files","text":"<p>If you want to convert many image files from <code>png</code> to <code>jpg</code> for example, you can type the following command using mogrify. <code>cd</code> into the folder where the images are situated and the the command: <pre><code>mogrify -format jpg *.png\n</code></pre></p> <p>Note that this will not delete the original files. You will need to run a <code>rm -rf *.png</code> to remove the original files in this example.</p>"},{"location":"FFMPEG%20%26%20ImageMagick/convert_video_to_gif/","title":"Convert Video to GIF","text":"<p>FFMPEG can output a high quality GIF from videos if you use the options well. It can also extremely fast with such a command: <pre><code>ffmpeg -ss 30 -t 3 -i input.mp4 -vf \"fps=10,scale=320:-1:flags=lanczos,split[s0][s1];[s0]palettegen[p];[s1][p]paletteuse\" -loop 0 output.gif\n</code></pre></p> <ul> <li>This example will skip the first 30 seconds (<code>-ss 30</code>) of the input and create a 3 second output (<code>-t 3</code>).</li> <li><code>fps</code> filter sets the frame rate. A rate of 10 frames per second is used in the example.</li> <li><code>scale</code> filter will resize the output to 320 pixels wide and automatically determine the height while preserving the aspect ratio. The lanczos scaling algorithm is used in this example.</li> <li><code>palettegen</code> and <code>paletteuse</code> filters will generate and use a custom palette generated from your input. These filters have many options, so refer to the links for a list of all available options and values.</li> <li><code>split</code> filter will allow everything to be done in one command and avoids having to create a temporary PNG file of the palette.</li> <li>Control looping with <code>-loop</code> output option but the values are confusing. A value of 0 is infinite looping, -1 is no looping, and 1 will loop once meaning it will play twice. So a value of 10 will cause the GIF to play 11 times.</li> </ul>"},{"location":"FFMPEG%20%26%20ImageMagick/convert_video_to_gif/#imagemagick-convert-example","title":"ImageMagick <code>convert</code> Example","text":"<p>Another command-line method is to pipe from <code>ffmpeg</code> to <code>convert</code> (or <code>magick</code>) from ImageMagick.</p> <pre><code>ffmpeg -i input.mp4 -vf \"fps=10,scale=320:-1:flags=lanczos\" -c:v pam -f image2pipe - | convert -delay 10 - -loop 0 -layers optimize output.gif\n</code></pre> <p>FFMPEG options:</p> <ul> <li><code>-vf \"fps=10,scale=320:-1:flags=lanczos\"</code> is a filtergraph using the fps and scale filters. fps sets frame rate to 10, and scale sets the size to 320 pixels wide and height is automatically determined and uses a value that preserves the aspect ratio. The lanczos scaling algorithm is used in this example.</li> <li><code>-c:v pam</code> Chooses the pam image encoder. The example outputs the PAM (Portable AnyMap) image format which is a simple, lossless RGB format that supports transparency (alpha) and is supported by convert. It is faster to encode than PNG.</li> <li><code>-f image2pipe</code> chooses the image2pipe muxer because when outputting to a pipe FFMPEG needs to be told which muxer to use.</li> </ul> <p><code>convert</code> options:</p> <ul> <li><code>-delay</code> See Setting frame rate section below.</li> <li><code>-loop 0</code> makes an infinite loop.</li> <li><code>-layers optimize</code> Will enable the general purpose GIF optimizer. See ImageMagick Animation Optimization for more details. It is not guaranteed that it will produce a smaller output, so it is worth trying without <code>-layers optimize</code> and comparing results.</li> </ul>"},{"location":"FFMPEG%20%26%20ImageMagick/convert_video_to_gif/#setting-framerate","title":"Setting Framerate","text":"<p>Set frame rate with a combination of the fps filter in <code>ffmpeg</code> and <code>-delay</code> in <code>convert</code>. This can get complicated because <code>convert</code> just gets a raw stream of images so no fps is preserved. Secondly, the <code>-delay</code> value in <code>convert</code> is in ticks (there are 100 ticks per second), not in frames per second. For example, with <code>fps=12.5</code> = 100/12.5 = 8 = <code>-delay 8</code>.</p> <p><code>convert</code> rounds the <code>-delay</code> value to a whole number, so 8.4 results in 8 and 8.5 results in 9. This effectively means that only some frame rates are supported when setting a uniform delay over all frames (a specific delay can be set per frame but that is beyond this answer).</p> <p><code>-delay</code> appears to be ignored if used as an output option, so it has to be used before - as shown in the example.</p> <p>Lastly, browsers and image viewers may implement a minimum delay, so your <code>-delay</code> may get ignored anyway.</p>"},{"location":"FFMPEG%20%26%20ImageMagick/cropping/","title":"Cropping Images and Video","text":"<p>Cropping images is something that is commonly done, however cropping many images at a time can become tedious. Furthermore cropping a video can be seen to be almost impossible without a non-linear video editing app at hand. This guide will show you the tools that imagemagick and ffmpeg provide to make thes tasks almost effortless.</p>"},{"location":"FFMPEG%20%26%20ImageMagick/cropping/#cropping-an-image-with-imagemagick","title":"Cropping an Image with ImageMagick","text":""},{"location":"FFMPEG%20%26%20ImageMagick/cropping/#removing-pixels-from-edge-of-image","title":"Removing Pixels from Edge of Image","text":"<p>Say we want to remove a certain number of pixel from each border of our image. Let's start by removing 200 pixels from the left and right of the image</p> <p>The command is the following:</p> <pre><code>convert original.jpg -shave 200x0 output.jpg\n</code></pre> <p>To remove 200 pixels from the top and bottom, the command is: <pre><code>convert original.jpg -shave 0x200 output.jpg\n</code></pre></p> <p>But what if we want different values for each side? Then we can combine two crop flags: <pre><code>                      #left,top      right,bottom\nconvert test.png -crop +180+140 -crop -60-140 cropped.png\n</code></pre></p>"},{"location":"FFMPEG%20%26%20ImageMagick/cropping/#removing-white-regions-of-an-image","title":"Removing White Regions of an Image","text":"<p>If you just want to remove the white regions of an image, ImageMagick has a dedicated command for that: <pre><code>convert -trim input.jpg output.jpg\n</code></pre></p> <p>You can also use: <pre><code>mogrify -trim file.jpg\n</code></pre> if you don't want to produce a copy of the original file.</p>"},{"location":"FFMPEG%20%26%20ImageMagick/cropping/#remove-transparent-border-of-image","title":"Remove Transparent Border of Image","text":"<p>If you ever stumble upon a png file with a large transparent border, you add in the <code>+repage</code> flag to remove the transparent border aswell: <pre><code>convert input.png -trim +repage output.png\n</code></pre></p>"},{"location":"FFMPEG%20%26%20ImageMagick/cropping/#cropping-a-video-with-ffmpeg","title":"Cropping a Video with FFMPEG","text":"<p>Cropping videos in FFMPEG requires a little more thinking, let's start with the command: <pre><code>ffmpeg -i input.mp4 -filter:v \"crop=w:h:x:y\" output.mp4\n</code></pre></p> <p>What this all means:</p> <ul> <li><code>-i input.mp4</code> specifies the input video (<code>input.mp4</code> being the input / original video in this case)</li> <li><code>-filter:v</code> (can be abbreviated to <code>-vf</code>) specifies we're using a video filter</li> <li> <p><code>\"crop=W:H:X:Y\"</code> means we're using the <code>\"crop\"</code> video filter, with 4 values:</p> <ul> <li><code>w</code> the width of the output video (so the width of the cropped region), which defaults to the input video width (input video width = <code>iw</code>, which is the same as <code>in_w</code>); out_w may also be used instead of <code>w</code></li> <li><code>h</code> the height of the output video (the height of the cropped region), which defaults to the input video height (input video height = <code>ih</code>, with <code>in_h</code> being another notation for the same thing); <code>out_h</code> may also be used instead of <code>h</code></li> <li><code>x</code> the horizontal position from where to begin cropping, starting from the left (with the absolute left margin being <code>0</code>)</li> <li><code>y</code> the vertical position from where to begin cropping, starting from the top of the video (the absolute top being <code>0</code>)</li> </ul> </li> <li> <p><code>output.mp4</code> is the new, cropped video file.</p> </li> </ul> <p>A few notes:</p> <ul> <li>The filter will automatically center the crop if <code>x</code> and <code>y</code> are omitted, so <code>x</code> defaults to <code>(iw-w)/2</code>, and <code>y</code> to <code>(ih-h)/2</code></li> <li>There is also an optional <code>keep_aspect</code> option that you can set to <code>1</code> to force the output display aspect ratio to be the same of the input (example usage: <code>\"crop=100:100:0:0:keep_aspect=1\"</code>). This won't work with images, that's why you don't see a separate example with screenshot here.</li> <li>FFmpeg gets the original input video width (<code>iw</code>) and height (<code>ih</code>) values automatically, so you can perform mathematical operations using those values (e.g. <code>iw/2</code> for half the input video width, or <code>ih-100</code> to subtract 100 pixels from the input video height).</li> </ul>"},{"location":"FFMPEG%20%26%20ImageMagick/split_gif_into_images/","title":"How to Split an Animated GIF into Frames","text":"<p>Type in the command: <pre><code>convert input.gif frame%4d.png\n</code></pre></p> <p>You can then manipulate each frame using imagemagick, and reassemble the frames into a gif using the first command in this guide.</p>"},{"location":"FFMPEG%20%26%20ImageMagick/transcode_video_resolve/","title":"Transcode Video Footage to make Compatible for Resolve","text":"<p>For a single file you can use:</p> <pre><code>ffmpeg -i GOPROxxx.MP4 -vcodec mjpeg -q:v 2 -acodec pcm_s16be -q:a 0 -f mov OUTxxx.mov\n</code></pre> <p>For a directory with many files, you can use :</p> <pre><code>mkdir transcoded; for i in *.mp4; do ffmpeg -i \"$i\" -vcodec mjpeg -q:v 2 -acodec pcm_s16be -q:a 0 -f mov \"transcoded/${i%.*}.mov\"; done\n</code></pre>"},{"location":"FFMPEG%20%26%20ImageMagick/yt_dlp/","title":"YT-DLP Guide","text":"<p><code>yt-dlp</code> is a simple terminal tool that allows you to download videos from YouTube onto your computer.</p>"},{"location":"FFMPEG%20%26%20ImageMagick/yt_dlp/#installation","title":"Installation","text":"<p>Installing the tool is easy, you can do it with a single command on all Unix systems:</p> UbuntuArchmacOS <pre><code>sudo apt install yt-dlp\n</code></pre> <pre><code>sudo pacman -S yt-dlp\n</code></pre> <pre><code>brew install yt-dlp\n</code></pre>"},{"location":"FFMPEG%20%26%20ImageMagick/yt_dlp/#downloading-a-youtube-video","title":"Downloading a YouTube Video","text":"<p>You can download a youtube video with the simple command: <pre><code>yt-dlp \"&lt;url-of-video&gt;\"\n</code></pre></p> <p>There are other options for downloading YouTube videos with this command line tool: - Download the best format (video + audio) that is equal to or greater than 720p width. Save this file as video_id.extension (<code>1La4QzGeaaQ.mp4</code>):     <pre><code>yt-dlp -f \"best[height&gt;=720]\" \"&lt;video-url&gt;\" -o '%(id)s.%(ext)s'\n</code></pre></p> <ul> <li> <p>Download and merge the best video stream with the best audio stream:     <pre><code>yt-dlp -f 'bv*+ba' \"&lt;video-url&gt;\" -o '%(id)s.%(ext)s'\n</code></pre></p> </li> <li> <p>Download 1080p video and merge with best audio stream:     <pre><code>yt-dlp -f 'bv*[height=1080]+ba' \"&lt;video-url&gt;\" -o '%(id)s.%(ext)s'\n</code></pre></p> </li> <li> <p>Download 1080p video that is mp4 format and merge with best m4a audio format:     <pre><code>yt-dlp -f 'bv[height=1080][ext=mp4]+ba[ext=m4a]' --merge-output-format mp4 \"&lt;video-url&gt;\" -o '%(id)s.mp4'\n</code></pre></p> </li> <li> <p>Embed video thumbnail into video file using <code>--embed-thumbnail</code>:     <pre><code>yt-dlp -f 'bv[height=1080][ext=mp4]+ba[ext=m4a]' --embed-thumbnail --merge-output-format mp4 \"&lt;video-url&gt;\" -o '%(id)s.mp4'\n</code></pre></p> </li> <li> <p>Embed subtitles to video file (if they exist) using <code>--embed-subs</code>:     <pre><code>yt-dlp -f 'bv[height=1080][ext=mp4]+ba[ext=m4a]' --embed-subs --merge-output-format mp4 \"&lt;video-url&gt;\" -o '%(id)s.mp4'\n</code></pre></p> </li> <li> <p>Embed metadata about the video using <code>--embed-metadata</code>:     <pre><code>yt-dlp -f 'bv[height=1080][ext=mp4]+ba[ext=m4a]' --embed-metadata --merge-output-format mp4 \"&lt;video-url&gt;\" -o '%(id)s.mp4'\n</code></pre></p> </li> </ul> <p>The video will then be downloaded to the directory where you run the command in.</p>"},{"location":"FFMPEG%20%26%20ImageMagick/yt_dlp/#downloading-audio-only-from-youtube-video","title":"Downloading Audio only from YouTube Video","text":"<p>There are flags to add to your command if you want to download the audio only from a video: - As you will more than likely be wanting the best quality audio the format (<code>-f</code>) selector will be for best audio (<code>ba</code>). - To save the highest quality audio as an mp3 file you need to define <code>--audio-format mp3</code> with <code>-x</code> which is extract audio.</p> <pre><code>yt-dlp -f 'ba' -x --audio-format mp3 \"&lt;video-url&gt;\"  -o '%(id)s.%(ext)s'\n</code></pre>"},{"location":"apps/install_etcher/","title":"Install Etcher","text":"UbuntuArchmacOS <ol> <li> <p>Add Etcher debian repository:</p> <p>In command terminal fire the below command to add the Etcher repository:</p> <pre><code>echo \"deb https://deb.etcher.io stable etcher\" | sudo tee /etc/apt/sources.list.d/balena-etcher.list\n</code></pre> <p>Import the GPG key:</p> <pre><code>sudo apt-key adv --keyserver keyserver.ubuntu.com --recv-keys 379CE192D401AB61\n</code></pre> </li> <li> <p>Update the system:</p> <pre><code>sudo apt-get update\n</code></pre> </li> <li> <p>Install Etcher on Ubuntu:</p> <pre><code>sudo apt-get install balena-etcher-electron\n</code></pre> </li> </ol> <p>X. Uninstall Etcher:     <pre><code>sudo apt-get remove balena-etcher-electron\nsudo rm /etc/apt/sources.list.d/balena-etcher.list\nsudo apt-get update\n</code></pre></p> <p>Install package from the AUR: <pre><code>sudo yay -S balena-etcher\n</code></pre></p> <p>Install package using Homebrew: <pre><code>brew install balenaetcher\n</code></pre></p>"},{"location":"apps/linux_setup_bluetooth/","title":"Set Up Bluetooth in Linux","text":"<ol> <li> <p>Start by installing bluez*. It adds a bunch of drivers and will most likely be the reason why your headset works now.    <pre><code> sudo apt-get install bluez*\n</code></pre></p> </li> <li> <p>Install blueman, this will help connect to your headphones.    <pre><code>sudo apt-get install blueman\n</code></pre></p> </li> <li> <p>Go into the file <code>/etc/bluetooth/main.conf</code> <pre><code>sudo vim /etc/bluetooth/main.conf\n</code></pre></p> </li> <li> <p>Change the following lines:    <pre><code>ControllerMode = bredr\n</code></pre>    or    <pre><code>ControllerMode = dual\n</code></pre></p> </li> <li> <p>Save the file and exit, then run the command:    <pre><code>sudo /etc/init.d/bluetooth restart\n</code></pre></p> </li> </ol>"},{"location":"apps/network_scan/","title":"Scan for IP Addresses on Local Network","text":"<p>Using the <code>arp-scan</code>, you can easily get a list of all of the IP addresses of the devices connected to the local network.</p>"},{"location":"apps/network_scan/#installing-utility","title":"Installing Utility","text":"<p>You can easily install <code>arp-scan</code> with the following commands:</p> UbuntumacOS <pre><code>sudo apt install arp-scan\n</code></pre> <pre><code>brew install arp-scan\n</code></pre>"},{"location":"apps/network_scan/#using-tool","title":"Using Tool","text":"<p>After installing, you can use the following command to output the IP addresses:</p> <pre><code>sudo arp-scan --localnet\n</code></pre> <p>You should then get a list of all IP addresses on your local network.</p> <p>For more information on <code>arp-scan</code>, you can run the following command:</p> <pre><code>arp-scan --help\n</code></pre> <p>or checkout the documentation on the app's github page.</p>"},{"location":"coding/asynchronous_programming/","title":"Asynchronous Programming","text":"<p>Info</p> <p>This guide is written from the video How To Easily Do Asynchronous Programming With Asyncio In Python.</p> <p>The code for which can be found here.</p> <p>This example shows you how to use the <code>asyncio</code> module in python to asynchronously run code (run in parallel) on your computer. To help guide us through this, we will use an example of controlling three IOT devices, a smart light bulb, a smart speaker and a smart toilet.</p> <code>main.py</code><code>iot/devices.py</code><code>iot/service.py</code><code>iot/message.py</code> <pre><code>import asyncio\nfrom typing import Any, Awaitable\nfrom iot.devices import HueLightDevice, SmartSpeakerDevice, SmartToiletDevice\nfrom iot.message import Message, MessageType\nfrom iot.service import IOTService\nasync def run_sequence(*functions: Awaitable[Any]) -&gt; None:\nfor function in functions:\nawait function\nasync def run_parallel(*functions: Awaitable[Any]) -&gt; None:\nawait asyncio.gather(*functions)\nasync def main() -&gt; None:\n# create a IOT service\nservice = IOTService()\n# create and register a few devices\nhue_light = HueLightDevice()\nspeaker = SmartSpeakerDevice()\ntoilet = SmartToiletDevice()\nhue_light_id, speaker_id, toilet_id = await asyncio.gather(\nservice.register_device(hue_light),\nservice.register_device(speaker),\nservice.register_device(toilet),\n)\n# create a few programs\nwake_up_program = [\nMessage(hue_light_id, MessageType.SWITCH_ON),\nMessage(speaker_id, MessageType.SWITCH_ON),\nMessage(speaker_id, MessageType.PLAY_SONG, \"Miles Davis - Kind of Blue\"),\n]\n# run the programs\nawait service.run_program(wake_up_program)\nawait run_parallel(\nservice.send_msg(Message(hue_light_id, MessageType.SWITCH_OFF)),\nservice.send_msg(Message(speaker_id, MessageType.SWITCH_OFF)),\nrun_sequence(\nservice.send_msg(Message(toilet_id, MessageType.FLUSH)),\nservice.send_msg(Message(toilet_id, MessageType.CLEAN)),\n),\n)\nif __name__ == \"__main__\":\nasyncio.run(main())\n</code></pre> <pre><code>import asyncio\nfrom iot.message import MessageType\nclass HueLightDevice:\nasync def connect(self) -&gt; None:\nprint(\"Connecting Hue Light.\")\nawait asyncio.sleep(0.5)\nprint(\"Hue Light connected.\")\nasync def disconnect(self) -&gt; None:\nprint(\"Disconnecting Hue Light.\")\nawait asyncio.sleep(0.5)\nprint(\"Hue Light disconnected.\")\nasync def send_message(self, message_type: MessageType, data: str = \"\") -&gt; None:\nprint(\nf\"Hue Light handling message of type {message_type.name} with data [{data}].\"\n)\nawait asyncio.sleep(0.5)\nprint(\"Hue Light received message.\")\nclass SmartSpeakerDevice:\nasync def connect(self) -&gt; None:\nprint(\"Connecting to Smart Speaker.\")\nawait asyncio.sleep(0.5)\nprint(\"Smart Speaker connected.\")\nasync def disconnect(self) -&gt; None:\nprint(\"Disconnecting Smart Speaker.\")\nawait asyncio.sleep(0.5)\nprint(\"Smart Speaker disconnected.\")\nasync def send_message(self, message_type: MessageType, data: str = \"\") -&gt; None:\nprint(\nf\"Smart Speaker handling message of type {message_type.name} with data [{data}].\"\n)\nawait asyncio.sleep(0.5)\nprint(\"Smart Speaker received message.\")\nclass SmartToiletDevice:\nasync def connect(self) -&gt; None:\nprint(\"Connecting to Smart Toilet.\")\nawait asyncio.sleep(0.5)\nprint(\"Smart Toilet connected.\")\nasync def disconnect(self) -&gt; None:\nprint(\"Disconnecting Smart Toilet.\")\nawait asyncio.sleep(0.5)\nprint(\"Smart Toilet disconnected.\")\nasync def send_message(self, message_type: MessageType, data: str = \"\") -&gt; None:\nprint(\nf\"Smart Toilet handling message of type {message_type.name} with data [{data}].\"\n)\nawait asyncio.sleep(0.5)\nprint(\"Smart Toilet received message.\")\n</code></pre> <pre><code>import asyncio\nimport random\nimport string\nfrom typing import Any, Awaitable, Protocol\nfrom iot.message import Message, MessageType\ndef generate_id(length: int = 8):\nreturn \"\".join(random.choices(string.ascii_uppercase, k=length))\nclass Device(Protocol):\nasync def connect(self) -&gt; None:\n...\nasync def disconnect(self) -&gt; None:\n...\nasync def send_message(self, message_type: MessageType, data: str = \"\") -&gt; None:\n...\nclass IOTService:\ndef __init__(self):\nself.devices: dict[str, Device] = {}\nasync def register_device(self, device: Device) -&gt; str:\nawait device.connect()\ndevice_id = generate_id()\nself.devices[device_id] = device\nreturn device_id\nasync def unregister_device(self, device_id: str) -&gt; None:\nawait self.devices[device_id].disconnect()\ndel self.devices[device_id]\ndef get_device(self, device_id: str) -&gt; Device:\nreturn self.devices[device_id]\nasync def run_program(self, program: list[Message]) -&gt; None:\nprint(\"=====RUNNING PROGRAM======\")\nawait asyncio.gather(*[self.send_msg(msg) for msg in program])\nprint(\"=====END OF PROGRAM======\")\nasync def send_msg(self, msg: Message) -&gt; None:\nawait self.devices[msg.device_id].send_message(msg.msg_type, msg.data)\n</code></pre> <pre><code>from dataclasses import dataclass\nfrom enum import Enum, auto\nclass MessageType(Enum):\nSWITCH_ON = auto()\nSWITCH_OFF = auto()\nCHANGE_COLOR = auto()\nPLAY_SONG = auto()\nOPEN = auto()\nCLOSE = auto()\nFLUSH = auto()\nCLEAN = auto()\n@dataclass\nclass Message:\ndevice_id: str\nmsg_type: MessageType\ndata: str = \"\"\n</code></pre>"},{"location":"coding/bad_python_habits/","title":"Bad Python Habits","text":"<p>This page goes over 25 common bad habits that a new programmer commonly does when starting out with Python. While some of these habits can impact your code in a bad way, some will also work fine, albeit not as efficiently as the proper method.</p> <p>This page is inspired from the video 25 Nooby Python Habits You Need to Ditch.</p>"},{"location":"coding/bad_python_habits/#1-manual-string-formatting","title":"1. Manual String Formatting","text":"<pre><code>def manual_str_formatting(name, subscribers):\n# Not great\nif subscribers &gt; 100000:\nprint(\"Wow \" + name + \"! you have \" + str(subscribers) + \" subscribers!\")\nelse:\nprint(\"Lol \" + name + \" that's not many subs\")\n# Better\nif subscribers &gt; 100000:\nprint(f\"Wow {name}! you have {subscribers} subscribers!\")\nelse:\nprint(f\"Lol {name} that's not many subs\")\n</code></pre> <p>Using <code>fstring</code>s are easier to write, less prone to errors, and easier to read.</p>"},{"location":"coding/bad_python_habits/#2-manually-closing-a-file","title":"2. Manually Closing a File","text":"<pre><code>def manually_calling_close_on_a_file(filename):\n# Not great\nf = open(filename, \"w\")\nf.write(\"hello!\\n\")\nf.close()\n# Better - close automatic, even if exception\nwith open(filename, \"w\") as f:\nf.write(\"hello!\\n\")\n</code></pre> <p>With the first method, if <code>f.write(\"hello!\\n\")</code> throws an exception, the file will never close. The resource manager will close the file even when an exception is thrown.</p>"},{"location":"coding/bad_python_habits/#3-using-try-and-finally-instead-of-a-context-manager","title":"3. Using <code>try:</code> and <code>finally:</code> Instead of a Context Manager","text":"<pre><code>def finally_instead_of_context_manager(host, port):\n# Not great\ns = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\ntry:\ns.connect((host, port))\ns.sendall(b'Hello, world')\nfinally:\ns.close()\n# Better - close even if exception\nwith socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\ns.connect((host, port))\ns.sendall(b'Hello, world')\n</code></pre> <p>In python, most resources that need to be closed have a context manager, use it!</p>"},{"location":"coding/bad_python_habits/#4-using-a-bare-except-clause","title":"4. Using a Bare <code>except:</code> Clause","text":"<pre><code>def bare_except():\n# Not great\nwhile True:\ntry:\ns = input(\"Input a number: \")\nx = int(s)\nbreak\nexcept:  # oops! can't CTRL-C to exit\nprint(\"Not a number, try again\")\n# Better\nwhile True:\ntry:\ns = input(\"Input a number: \")\nx = int(s)\nbreak\nexcept Exception:\nprint(\"Not a number, try again\")\n# Even Better\nwhile True:\ntry:\ns = input(\"Input a number: \")\nx = int(s)\nbreak\nexcept ValueError:  # Here, we catch the actual exception that's going to be thrown\nprint(\"Not a number, try again\")\n</code></pre> <p>In python, a bare except is usually going to catch something like hitting Ctrl+C, which is something you almost never want to do.</p>"},{"location":"coding/bad_python_habits/#5-thinking-that-means-exponentiation","title":"5. Thinking that <code>^</code> Means Exponentiation","text":"<pre><code>def caret_and_exponentiation(x, p):\ny = x ^ p  # Bitwise xor of x and p, not exponentiation\ny = x ** p # Exponentiation\n</code></pre>"},{"location":"coding/bad_python_habits/#6-use-of-default-mutable-arguments","title":"6. Use of Default Mutable Arguments","text":"<pre><code>def mutable_default_arguments():\n# Not great\ndef append(n, l=[]):\nl.append(n)\nreturn l\nl1 = append(0)  # [0]\nl2 = append(1)  # [0, 1]\n# Better\ndef append(n, l=None):\nif l is None:\nl = []\nl.append(n)\nreturn l\nl1 = append(0)  # [0]\nl2 = append(1)  # [1]\n</code></pre> <p>Any use of default mutable arguments is to be avoided. Argument defaults are defined when the function is defined, not when it's run. In the first method, every and any call to the function <code>append</code> is sharing the same list!</p> <p>If you want to define a default, set it to <code>None</code> and then check if it's None and set the default there.</p>"},{"location":"coding/bad_python_habits/#7-never-using-comprehensions-or-only-using-list-comprehensions","title":"7. Never Using Comprehensions, or Only Using List Comprehensions","text":"<pre><code>def never_using_comprehensions():\n# Not great\nsquares = {}\nfor i in range(10):\nsquares[i] = i * i\n# Better\nodd_squares = {i: i * i for i in range(10)}\n</code></pre> <p>A lot of code can be made shorter and clearer by using a comprehension. You can have dictionary, lists, set and even generator comprehensions (see below). You can learn more about comprehensions here.</p> <pre><code>def example_comprehensions():\ndict_comp = [i * i for i in range(10)]\nlist_comp = [x * x for x in range(10)]\nset_comp = [i%3 for i in range(10)]\ngen_comp = [2*x+5 for x in range(10)]\n</code></pre>"},{"location":"coding/bad_python_habits/#8-always-using-comprehensions","title":"8. ALWAYS using comprehensions","text":"<pre><code>def always_using_comprehensions(a, b, n):\n\"\"\"matrix product of a, b of length n x n\"\"\"\n# Not always needed\nc = [\nsum(a[n * i + k] * b[n * k + j] for k in range(n))\nfor i in range(n)\nfor j in range(n)\n]\n# More readable\nc = []\nfor i in range(n):\nfor j in range(n):\nij_entry = sum(a[n * i + k] * b[n * k + j] for k in range(n))\nc.append(ij_entry)\nreturn \n</code></pre> <p>Of course, you can flex with your comprehensions, but you don't need to turn every single loop into a comprehension. Think about what is more readable and reasonable to use.</p>"},{"location":"coding/bad_python_habits/#9-checking-for-a-type-using","title":"9. Checking for a type using <code>==</code>","text":"<pre><code>def checking_type_equality():\nPoint = namedtuple('Point', ['x', 'y'])\np = Point(1, 2)\n# Not great\nif type(p) == tuple:\nprint(\"it's a tuple\")\nelse:\nprint(\"it's not a tuple\")\n# Better\nif isinstance(p, tuple):    # Probably meant to check if is instance of tuple\nprint(\"it's a tuple\")\nelse:\nprint(\"it's not a tuple\")\n</code></pre> <p>While there are some rare cases where you do want to do this, but most of the time, this is not what you want. The reason is inheritance. In the example above, a namedTuple is a tuple so the <code>Point</code> class is a tuple, but it's not literally a tuple, it is a subclass</p>"},{"location":"coding/bad_python_habits/#10-using-to-check-for-none-true-or-false","title":"10. Using <code>==</code> to check for <code>None</code>, <code>True</code>, or <code>False</code>","text":"<pre><code>def equality_for_singletons(x):\n# Not great\nif x == None:\npass\nif x == True:\npass\nif x == False:\npass\n# Better\nif x is None:\npass\nif x is True:\npass\nif x is False:\npass\n</code></pre> <p>Instead of equality, you should check for identity using the <code>is</code> comparison. This is what <code>==</code> was going to use anyway, so cut out the middle-man and use <code>is</code> directly.</p>"},{"location":"coding/bad_python_habits/#11-using-an-if-boolx-or-if-lenx-check","title":"11. Using an <code>if bool(x):</code> or <code>if len(x)</code> check","text":"<pre><code>def checking_bool_or_len(x):\n# Not great\nif bool(x):\npass\nif len(x) != 0:\npass\n# Better - Expressions above are usually equivalent to this\nif x:\npass\n</code></pre>"},{"location":"coding/bad_python_habits/#12-using-the-rangelena-idiom","title":"12. Using the <code>range(len(a))</code> idiom","text":"<pre><code>def range_len_pattern():\na = [1, 2, 3]\n# Not great\nfor i in range(len(a)):\nv = a[i]\n...\n# Better - Range loop\nfor v in a:\n...\n# Or if you wanted the index\nfor i, v in enumerate(a):\n...\n# Using i to sync between two things?\nb = [4, 5, 6]\n# Not great\nfor i in range(len(b)):\nav = a[i]\nbv = b[i]\n...\n# Better - Instead use zip\nfor av, bv in zip(a, b):\n...\n# If you still need the index\nfor i, (av, bv) in enumerate(zip(a, b)):\n...\n</code></pre>"},{"location":"coding/bad_python_habits/#13-looping-over-the-keys-of-a-dictionary","title":"13. Looping over the keys of a dictionary","text":"<pre><code>def for_key_in_dict_keys():\nd = {\"a\": 1, \"b\": 2, \"c\": 3}\nfor key in d.keys():\n...\n# That's the default\nfor key in d:\n...\n# Or if you meant to make a copy of keys\nfor key in list():\n...\n</code></pre>"},{"location":"coding/bad_python_habits/#14-not-knowing-about-the-dictionary-items-methods","title":"14. Not knowing about the dictionary items methods","text":"<pre><code>def not_using_dict_items():\nd = {\"a\": 1, \"b\": 2, \"c\": 3}\n# Not great\nfor key in d:\nval = d[key]\n...\n# Better\nfor key, val in d.items():\n...\n</code></pre>"},{"location":"coding/bad_python_habits/#15-not-using-tuple-unpacking","title":"15. Not using tuple unpacking","text":"<pre><code>def tuple_unpacking():\nx = 0\ny = 1\ntmp = x\nx = y\ny = tmp\nx, y = 0, 1\nx, y = y, x\nmytuple = 1, 2\n# Not great\nx = mytuple[0]\ny = mytuple[1]\n# Better\nx, y = mytuple\n</code></pre> <p>If you have a tuple and want to get both of it's elements out in seprate variables? Use tuple unpacking!</p>"},{"location":"coding/bad_python_habits/#16-creating-your-own-index-counter-variable","title":"16. Creating your own index counter variable","text":"<pre><code>def index_counter_variable():\nl = [1, 2, 3]\n# Not great\ni = 0\nfor x in l:\n...\ni += 1\n# Better\nfor i, x in enumerate(l):\n...\n</code></pre>"},{"location":"coding/bad_python_habits/#17-using-timetime-to-time-things","title":"17. Using <code>time.time()</code> to time things","text":"<pre><code>def timing_with_time():\n# Not great\nstart = time.time()\ntime.sleep(1)\nend = time.time()\nprint(end - start)\n# Better - More accurate\nstart = time.perf_counter()\ntime.sleep(1)\nend = time.perf_counter()\nprint(end - start)\n</code></pre> <p><code>time.time()</code> is for telling you what time it currently is, and it's not as accurate as <code>time.perf_counter()</code>.</p>"},{"location":"coding/bad_python_habits/#18-not-using-the-logging-module","title":"18. Not using the logging module","text":"<pre><code>def print_vs_logging():\n# To be avoided\nprint(\"debug info\")\nprint(\"just some info\")\nprint(\"bad error\")\n# Better\n# in main\nlevel = logging.DEBUG\nfmt = '[%(levelname)s] %(asctime)s - %(message)s'\nlogging.basicConfig(level=level, format=fmt)\n# wherever\nlogging.debug(\"debug info\")\nlogging.info(\"just some info\")\nlogging.error(\"uh oh :(\")\n</code></pre> <p>You can set up your logging format the way you want it. You can also set up logging to filter out messages that you are not interested in.</p>"},{"location":"coding/bad_python_habits/#19-using-shell-true-on-any-function-in-the-subprocess-library","title":"19. Using <code>shell = True</code> on any function in the subprocess library","text":"<pre><code>def subprocess_with_shell_true():\n# Not great\nsubprocess.run([\"ls -l\"], capture_output=True, shell=True)\n# Better\nsubprocess.run([\"ls\", \"-l\"], capture_output=True)\n</code></pre> <p><code>shell=True</code> is the source of a lot of security problems. The most common reason for doing this is to avoid putting your arguments into a list.</p>"},{"location":"coding/bad_python_habits/#20-doing-maths-or-any-kind-of-data-analysis-in-python","title":"20. Doing maths or any kind of data analysis in Python","text":"<pre><code>def not_using_numpy_pandas():\n# Not great - slow\nx = list(range(100))\ny = list(range(100))\ns = [a + b for a, b in zip(x, y)]\n# Better - faster\nx = np.arange(100)\ny = np.arange(100)\ns = x + y\n</code></pre> <p>Learn to use numpy arrays for math operations, and learn to use pandas for more general data analysis. They run C or C++ in the background and will perform operations much faster that in Python.</p>"},{"location":"coding/bad_python_habits/#21-using-import-outside-of-an-interactive-session","title":"21. Using <code>import *</code> outside of an interactive session","text":"<pre><code>from itertools import *\n# who knows what variables are in scope now?\ncount()\n</code></pre> <p><code>import *</code> usually litters your namespace with variables. Instead, just import the things you actually need.</p>"},{"location":"coding/bad_python_habits/#22-depending-on-a-specific-directory-structure-for-your-project","title":"22. Depending on a specific directory structure for your project","text":"<pre><code>from mypackage.nearby_module import awesome_function\ndef main():\nawesome_function()\nif __name__ == '__main__':\nmain()\n</code></pre> <p>Relying on your folder structure to run code successfully makes your repo more fragile. Instead, take the time to create packages from the code you wrote and install it into your environment.</p>"},{"location":"coding/bad_python_habits/#23-emptying-a-list-by-assigning-a-new-value","title":"23. Emptying a list by assigning a new value","text":"<pre><code>def empty_list(list):\n# Not great\nlist = []\n# Better\ndel list[:]\n</code></pre> <p><code>del list[:]</code> actually removes the contents from the list, writing <code>list = []</code> does not empty the list, just creates a new object and binds it to the variable <code>list</code>, but the old list will still have the same elements, and effect will be apparent if it had other variable bindings.</p>"},{"location":"coding/bad_python_habits/#24-the-common-misconception-that-python-is-not-compiled","title":"24. The common misconception that Python is not compiled","text":"<p>Python isn't compiled down to machine code. Instead, it is compiled down to byte code. That byte is then run by the interpreter.</p> <p>You will commonly see these byte-code files in a <code>.pyc</code> file or a <code>__pycache__</code> file.</p>"},{"location":"coding/bad_python_habits/#25-not-following-pep-8","title":"25. Not following PEP 8","text":"<p>PEP 8 is a style guide for writing python code. It is the standard style and makes things a lot easier to read. You can learn more about PEP 8 here.</p>"},{"location":"coding/bad_python_habits/#26-doing-anything-to-do-with-python-2","title":"26. Doing anything to do with Python 2","text":"<p>Python 2 hit its end of life years ago (to the point that it's not supported by modern operating systems anymore). The only reason for keeping Python 2 code around is only if you have millions (and nothing less) of lines of python 2 code.</p>"},{"location":"coding/creating_venvs/","title":"Creating Virtual Environments in Python","text":""},{"location":"coding/creating_venvs/#introduction","title":"Introduction","text":"<p>When developing software with Python, a basic approach is to install Python on your machine, install all your required libraries via the terminal, write all your code in a single .py file or notebook, and run your Python program in the terminal.</p> <p>This is a common approach for a lot of beginners and many people transitioning from working with Python for data analytics.</p> <p>This works fine for simple Python scripting projects. But in complex software development projects, like building a Python library, an API, or software development kit, often you will be working with multiple files, multiple packages, and dependencies. As a result, you will need to isolate your Python development environment for that particular project.</p> <p>To solve this problem, we can use virtual environments. This tutorial will cover everything on how to set one up with <code>pip</code> or <code>conda</code>.</p> PipConda <p>This part is still under construction!</p>"},{"location":"coding/creating_venvs/#installing-virtualenv-tool","title":"Installing <code>virtualenv</code> Tool","text":"<p>Open your command prompt, enter the following command:</p> <pre><code>pip install virtualenv\n</code></pre>"},{"location":"coding/creating_venvs/#creating-virtual-environment-for-your-project","title":"Creating Virtual Environment for your project","text":"<p>To create a new virtual environment with a specific python version, type in the following command shown below. In the example below, we will create a new virtual environment called <code>env</code> in Python 3.8. <pre><code>python3.8 -m venv env\n</code></pre></p>"},{"location":"coding/creating_venvs/#configuring-git-with-virtual-environments","title":"Configuring Git with Virtual Environments","text":"<p>Creating a new virtual environment creates a new folder in your project. However this folder can contain a lot of data that is not universal and can clog up your git repo. To avoid this, add the following line to your <code>.gitignore</code> file: <pre><code>env/lib\n</code></pre></p>"},{"location":"coding/creating_venvs/#activate-virtual-environment","title":"Activate Virtual Environment","text":"<p>Enter the following command. As with the example above, our virtual environment is called <code>env</code>: <pre><code>source env/bin/activate\n</code></pre></p>"},{"location":"coding/creating_venvs/#deactivatingexiting-virtual-environment","title":"Deactivating/Exiting Virtual Environment","text":"<p>You can simply type in the command to exit your virtual environment: <pre><code>deactivate\n</code></pre></p>"},{"location":"coding/diagnosing_slow_python/","title":"Diagnosing Slow Python Code","text":"<p>Info</p> <p>This guide is inspired from the video Diagnose Slow Python Sode (feat. async/await)</p> <p>Imagine we have a bit of code in python and it's performance is under the required specififcation. We want an effective way to find what in the code is causing the slowdown without guessing or relying on any experience of the coder. A wrong guess can mean spending hours of time improving a bit of code only to find that performance is barely improved.</p> <p>In our example, say we have a function that we run from the <code>main()</code> function:</p> <pre><code>import ...\ndef does_stuff():\n...\ndef does_stuff_slow():\n...\ndef main():\ndoes_stuff()\ndoes_stuff_slow()\nif __name__ == '__main__':\nmain()\n</code></pre> <p>We can use <code>cProfile</code> and <code>snakeviz</code> to show us what is slowing down the code in our project. Both can be install via <code>pip</code>.</p>"},{"location":"coding/diagnosing_slow_python/#using-cprofile","title":"Using <code>cProfile</code>","text":"<p>Using <code>cProfile</code> for debugging slow code is easy. All we need to do is add the following lines to our main function:</p> <pre><code>import ...\ndef does_stuff():\n...\ndef does_stuff_slow():\n...\ndef main():\nimport cProfile\nimport pstats\ndoes_stuff()\nwith cProfile.Profile() as pr:          # Profile the slow function\ndoes_stuff_slow()\nstats = pstats.Stats(pr)                # Create stats object from profile\nstats.sort_stats(pstats.SortKey.TIME)   # Sort stats by the total time they took\nstats.print_stats()                     # Print the sorted stats in the terminal\nif __name__ == '__main__':\nmain()\n</code></pre> <p>When running this, we will see in the terminal all of the operations and how long they took. This will already give us a good idea of what is taking the most time to run, so we can focus on that.</p>"},{"location":"coding/diagnosing_slow_python/#using-snakeviz","title":"Using <code>snakeviz</code>","text":"<p>However what we want to vizualize all of our operations in the code in the order that they are executed and how long they take? To do that we can use a tool called <code>snakeviz</code>.</p> <p>To use <code>snakeviz</code>, we jsut need to modify the code a little bit more:</p> <pre><code>import ...\ndef does_stuff():\n...\ndef does_stuff_slow():\n...\ndef main():\nimport cProfile\nimport pstats\ndoes_stuff()\nwith cProfile.Profile() as pr:          # Profile the slow function\ndoes_stuff_slow()\nstats = pstats.Stats(pr)                # Create stats object from profile\nstats.sort_stats(pstats.SortKey.TIME)   # Sort stats by the total time they took\n# stats.print_stats()                   # Print the sorted stats in the terminal\nstats.dump_stats(filename='needs_profiling.prof')   # Dump stats to a file\nif __name__ == '__main__':\nmain()\n</code></pre> <p>After running this code, the new file <code>needs_profiling.prof</code> will be generated. To view this in <code>snakeviz</code>, we type in the following command in the terminal:</p> <pre><code>snakeviz ./needs_profiling.prof\n</code></pre> <p><code>snakeviz</code> will then open up a new browser window where you can see clearly all of the operations and how long they take in your code.</p>"},{"location":"coding/install_pyenv_ubuntu/","title":"Install <code>pyenv</code> on Ubuntu","text":"<ol> <li> <p>Start by running verifying that <code>curl</code> and <code>git</code> are installed:     <pre><code>sudo apt update &amp;&amp; sudo apt upgrade -y git curl\n</code></pre></p> </li> <li> <p>Run the curl script:     <pre><code>curl https://pyenv.run | bash\n</code></pre></p> </li> <li> <p>Add the following lines to the <code>bashrc</code> file. Run <code>pyenv init</code> if you are not prompted to add these lines.     <pre><code>export PYENV_ROOT=\"$HOME/.pyenv\"\ncommand -v pyenv &gt;/dev/null || export PATH=\"$PYENV_ROOT/bin:$PATH\"\neval \"$(pyenv init -)\"\n</code></pre></p> </li> <li> <p>Restart the shell in Ubuntu:     <pre><code>exec $SHELL\n</code></pre></p> </li> <li> <p>Display the installed version of the pyenv:     <pre><code>pyenv --version\n</code></pre></p> </li> </ol> <p>Tip</p> <p>Depending on how fresh your installation of ubuntu is, you might need to run the following command to install the required packages to be able to build the different versions of python on your system:</p> <pre><code>sudo apt install -y build-essential zlib1g-dev libffi-dev libssl-dev libbz2-dev libreadline-dev libsqlite3-dev liblzma-dev\n</code></pre>"},{"location":"coding/publishing_python_library/","title":"Publishing a Python Package to PyPI","text":"<p>The Python Package Index (PyPI) has become the go-to place for searching and downloading python packages. This guide will take you through all of the steps to take a python project and upload it so that others may use it.</p>"},{"location":"coding/publishing_python_library/#preparing-project","title":"Preparing Project","text":""},{"location":"coding/publishing_python_library/#folder-structure","title":"Folder Structure","text":"<p>To help us illustrate how the folder structure is handled, let us use an example. Say we want to deploy the package with the name <code>&lt;package-name&gt;</code> to PyPI. The root folder of our repo is called <code>base</code> and inside, we should have the following structure:</p> <pre><code>.\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 README.md\n\u251c\u2500\u2500 demo/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 demo.py\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 &lt;package-name&gt;/\n\u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u251c\u2500\u2500 package_file.py\n\u2502   \u2514\u2500\u2500 ...\n\u251c\u2500\u2500 requirements.txt\n\u251c\u2500\u2500 setup.py\n\u2514\u2500\u2500 tests/\n    \u251c\u2500\u2500 __init__.py\n    \u251c\u2500\u2500 test_file.py\n    \u2514\u2500\u2500 ...\n</code></pre> <p>The core files of the library should all be within the <code>&lt;package-name&gt;/</code> folder. The <code>demo</code> and <code>tests</code> folders are places where you can add example scripts and tests if desired. The contents of the <code>setup.py</code> script is covered in the next section.</p> <p>The <code>__init__.py</code> files allow external scripts to 'see' the <code>&lt;package-name&gt;/</code> folder. Examples of the contents of these files are shown below:</p> <code>&lt;package-name&gt;/__init__.py</code><code>demo/__init__.py</code><code>tests/__init__.py</code> <pre><code>from .package_file import PackageClass\n# Add other files and their classes here\n</code></pre> <pre><code>from sys import path\nfrom os.path import dirname, join, abspath\npath.append(abspath(join(dirname(__file__), '..')))\n</code></pre> <pre><code>from sys import path\nfrom os.path import dirname, join, abspath\npath.append(abspath(join(dirname(__file__), '..')))\n</code></pre> <p>By setting the <code>__init__.py</code> files this way you can just call the package classes in a similar way to other packages:</p> <pre><code>from &lt;package_name&gt;.package_file import PackageClass\n</code></pre>"},{"location":"coding/publishing_python_library/#creating-setup-script","title":"Creating Setup Script","text":"<p>The setup script is what will be used to create the distribution that will be uploaded to PyPI. It should be located in the root directory of you repository. Below you can find a template of the <code>setup.py</code> script.</p> setup.py<pre><code>from setuptools import setup, find_packages\nVERSION = '1.1.0'\nDESCRIPTION = 'Transforming and handling poses.'\n# read the contents of your README file\nfrom pathlib import Path\nthis_directory = Path(__file__).parent\nlong_description = (this_directory / \"README.md\").read_text()\n# Add resource links\nproject_urls = {\n'Documentation': 'https://demo.org',\n'Repository': 'https://gitlab.com/'\n}\n# Setting up\nsetup(\nname=\"&lt;package-name&gt;\",\nversion=VERSION,\nauthor=\"Firstname Lastname\",\nauthor_email=\"&lt;email@address.com&gt;\",\ndescription=DESCRIPTION,\nlong_description=long_description,\nlong_description_content_type='text/markdown',\npackages=find_packages(),\ninstall_requires=open(\"requirements.txt\", \"r\").read().split(\"\\n\"),\nkeywords=['python'],\nproject_urls = project_urls,\nclassifiers=[\n\"Intended Audience :: Developers\",\n\"Programming Language :: Python :: 3\",\n\"Operating System :: Unix\",\n\"Operating System :: MacOS :: MacOS X\",\n\"Operating System :: Microsoft :: Windows\",\n]\n)\n</code></pre> <p>This setup script defines a few key parameters of the package to be published:</p> <ul> <li>Version number</li> <li>Which operating systems are allowed to install the package</li> <li>Dependencies of the package (defined the <code>requirements.txt</code> file)</li> <li>Adds links to documentation and repository (found in <code>project_urls</code>)</li> </ul> <p>This setup file is also configured to show the <code>README.md</code> file of the repository as the long description of the package of PyPI.</p> <p>Info</p> <p>Future improvements should be made to:</p> <ul> <li>Automatically increment the version number on a new build (using <code>bumpver</code>)</li> </ul>"},{"location":"coding/publishing_python_library/#publishing","title":"Publishing","text":""},{"location":"coding/publishing_python_library/#create-account-on-pypi","title":"Create Account on PyPi","text":"<p>If you don't already have an account on PyPi, you will need to create one (you can do so here). Make sure to save your username and password as you will need it for the following steps.</p>"},{"location":"coding/publishing_python_library/#publish-package","title":"Publish Package","text":"<p>Now that you project is ready, and you have an account on PyPI, we can now move onto publishing the package.</p>"},{"location":"coding/publishing_python_library/#building-project","title":"Building Project","text":"<p>Let us start by building the project, so that it is saved as an executable. Go to the root folder of your project (<code>base</code> in the example), and run the commands below:</p> <pre><code>rm -rf build dist *.egg-info\npython3 setup.py sdist bdist_wheel\n</code></pre> <p>Info</p> <p>Make sure to change the version number of your package in the <code>setup.py</code> file before building, otherwise PyPI will reject the new upload.</p> <p>The command with create three new folders:</p> <ul> <li><code>build</code></li> <li><code>dist</code></li> <li><code>&lt;package-name&gt;.egg-info</code></li> </ul> <p>The contents of the <code>dist</code> folder is what we will upload to PyPI.</p>"},{"location":"coding/publishing_python_library/#publishing-the-build-using-twine","title":"Publishing the Build using Twine","text":"<p>To publish this new build, we will use the <code>twine</code> python package (<code>pip3 install twine</code>).</p> <p>To upload, simply type in the following command:</p> <pre><code>twine upload dist/*\n</code></pre> <p><code>twine</code> will then ask for your PyPI username and password, and then upload the new build!</p>"},{"location":"coding/publishing_python_library/#adding-to-gitignore","title":"Adding to .gitignore","text":"<p>The command <code>python3 setup.py sdist bdist_wheel</code> creates three folders containing a lot of content. It is therefore best to add the following lines to your <code>.gitignore</code> file to avoid syncing these folders:</p> setup.py<pre><code>build/\ndist/\n*.egg-info/\n</code></pre>"},{"location":"file_handling/alternative_to_scp/","title":"Alternative to <code>scp</code> Command","text":"<p>The <code>scp</code> command has become outdated, and not as secure by today's standards. On this page we show an alternative to the <code>scp</code>, known as <code>rsync</code>. <code>rsync</code> is built into Unix systems now, and is built to be a fast and reliable way of copying files and folders to remote machines, you can read more about it here.</p>"},{"location":"file_handling/alternative_to_scp/#simple-copying","title":"Simple Copying","text":"<p>Simply copying a file can be done with the following command:</p> <pre><code>rsync &lt;source&gt; &lt;destination&gt;\n</code></pre>"},{"location":"file_handling/alternative_to_scp/#viewing-progress-when-syncing","title":"Viewing Progress When Syncing","text":"<p>You can view the progress of the copy operation by passing the <code>--progress</code>, <code>-P</code> flag. You will also need to turn non the verbose option aswell:</p> <pre><code>rsync -v --progress &lt;source&gt; &lt;destination&gt;\n</code></pre>"},{"location":"file_handling/alternative_to_scp/#recursive-copying","title":"Recursive Copying","text":"<p>Recursive copying is useful if you are copying an entire folder. You can do this with the <code>-r</code> flag:</p> <pre><code>rsync -rv --progress &lt;source&gt; &lt;destination&gt;\n</code></pre>"},{"location":"file_handling/alternative_to_scp/#other-flags-example","title":"Other Flags &amp; Example","text":"<p>The other flags and their descriptions for them can be found here.</p> <p>Below we show an example of a good <code>rsync</code> command:</p> <pre><code>rsync -rhv --progress ~/Downloads/some.pdf john@192.168.1.27:/home/Downloads/\n</code></pre>"},{"location":"file_handling/file_counting/","title":"Advanced File Counting in Terminal","text":""},{"location":"file_handling/file_counting/#count-number-of-files-and-directories-without-hidden-files","title":"Count Number of Files and Directories (without hidden files)","text":"<p>You can simply run the combination of the <code>ls</code> and <code>wc</code> command and it will display the number of files: <pre><code>ls | wc -l\n</code></pre></p>"},{"location":"file_handling/file_counting/#count-number-of-files-and-directories-including-hidden-files","title":"Count Number of Files and Directories (including hidden files)","text":"<p>You can simply run the combination of the <code>ls</code> and <code>wc</code> command with the <code>-A</code> flag and it will display the number of files (includeing the hidden files): <pre><code>ls -A | wc -l\n</code></pre></p>"},{"location":"file_handling/file_counting/#count-only-files-not-directories-or-subdirectories","title":"Count Only Files (not directories or subdirectories)","text":"<p>All you have to do is to add the <code>depth</code> of your find. If you set it at 1, it won\u2019t enter the subdirectories. <pre><code>find . -maxdepth 1 -type f | wc -l\n</code></pre></p>"},{"location":"file_handling/file_counting/#count-files-of-specific-type-in-this-directory","title":"Count files of Specific Type (in this directory)","text":"<p>Unfortunately this benign problem is difficult to solve in a way which supports all file names and is portable. This is safe (it handles hidden files, paths containing spaces, dashes and even newlines). In this example, we want to find all of the files with the <code>toml</code> extension: <pre><code>find . -mindepth 1 -maxdepth 1 -type f -name \"*.toml\" | uniq -c\n</code></pre></p>"},{"location":"file_handling/file_counting/#count-number-of-files-and-directories-including-the-subdirectories","title":"Count Number of Files and Directories (including the subdirectories)","text":"<p>If you want to count the number of files and directories in all the subdirectories, you can use the tree command: <pre><code>tree -a\n</code></pre></p>"},{"location":"file_handling/file_counting/#count-only-files-not-directories-including-subdirectories","title":"Count Only Files (not directories, including subdirectories)","text":"<p>The above command searched for all the files (type <code>f</code>) in current directory and its subdirectories. <pre><code>find . -type f | wc -l\n</code></pre></p>"},{"location":"file_handling/rename_many_files/","title":"Advanced File Renaming in Terminal","text":""},{"location":"file_handling/rename_many_files/#install-rename-utility","title":"Install <code>rename</code> Utility","text":"macOSUbuntuArch <pre><code>brew install rename\n</code></pre> <pre><code>sudo apt install rename\n</code></pre> <p>(Rename is built into Arch)</p>"},{"location":"file_handling/rename_many_files/#remove-first-n-characters-from-filenames","title":"Remove First n Characters from Filenames","text":"<p>To remove the first 8 characters from filename, type the following</p> <pre><code>rename -n -v 's/^(.{8})//' *\n</code></pre> <p><code>-n</code>is for no action and <code>-v</code>is to show the changes. If you are satisfied with the results you can remove the flags:</p> <pre><code>rename 's/^(.{8})//' *\n</code></pre>"},{"location":"file_handling/rename_many_files/#remove-last-n-characters-from-filenames-while-keeping-the-extension-you-want","title":"Remove Last n Characters from Filenames (while keeping the extension you want)","text":"<p>If we want to remove the last 29 characters from all of the mkv files in a folder, however we want to keep the mkv extension, we can write:</p> <pre><code>rename -n -v 's/.{29}$/.mkv/' *.mkv\n</code></pre> <p>Again, <code>-n</code> is for no action and <code>-v</code>is to show the changes. If you are satisfied with the results you can remove those two flags.</p>"},{"location":"file_handling/rename_many_files/#delete-part-of-a-filename","title":"Delete Part of a Filename","text":"<p>The <code>rename</code> option also allows you to delete a part of the filename by omitting the replacement part of the expression. For instance, if we want to shorten <code>example</code> into <code>ex</code>:</p> <pre><code>rename -v 's/ample//' *.txt\n</code></pre>"},{"location":"file_handling/rename_many_files/#rename-files-with-similar-names","title":"Rename Files with Similar Names","text":"<p>Another use for the <code>rename</code> option is to rename files with similar names. For instance, if we want to rename files with <code>example</code> and <code>sample</code> in their name to test:</p> <pre><code>rename -v 's/(ex|s)ample/test/' *.txt\n</code></pre>"},{"location":"operating_systems_%26_configs/get_windows/","title":"How to Get Windows","text":"<ol> <li> <p>Open CMD as Administrator</p> </li> <li> <p>Run the following command into the cmd (keep the quotation marks in):     <pre><code>cscript slmgr.vbs /ipk \"&lt;serial_number&gt;\"\n</code></pre></p> Tldr <p>Replace <code>&lt;serial_number&gt;</code> with the corresponding serial numbers below: <pre><code>Home/Core                            TX9XD-98N7V-6WMQ6-BX7FG-H8Q99        Home/Core (Country Specific)         PVMJN-6DFY6-9CCP6-7BKTT-D3WVR  Home/Core (Single Language)          7HNRX-D7KGG-3K4RQ-4WPJ4-YTDFH  Home/Core N                          3KHY7-WNT83-DGQKR-F7HPR-844BM Professional                         W269N-WFGWX-YVC9B-4J6C9-T83GX Professional N                       MH37W-N47XK-V7XM9-C7227-GCQG9 Enterprise                           NPPR9-FWDCX-D2C8J-H872K-2YT43 Enterprise N                         DPH2V-TTNVB-4X9Q3-TJR4H-KHJW4 Education                            NW6C2-QMPVW-D7KKK-3GKT6-VCFB2 Education N                          2WH4N-8QGBV-H22JP-CT43Q-MDWWJ Enterprise 2015 LTSB                 WNMTR-4C88C-JK8YV-HQ7T2-76DF9\nEnterprise 2015 LTSB N               2F77B-TNFGY-69QQF-B8YKP-D69TJ Enterprise 2016 LTSB                 DCPHK-NFMTC-H88MJ-PFHPY-QJ4BJ  Enterprise 2016 LTSB N               QFFDN-GRT3P-VKWWX-X7T3R-8B639\n</code></pre></p> </li> <li> <p>Run these last two commands to activate:     <pre><code>cscript slmgr.vbs /skms kms.lotro.cc\ncscript slmgr.vbs /ato\n</code></pre></p> </li> </ol>"},{"location":"operating_systems_%26_configs/graphics_drivers/","title":"Graphics Drivers","text":""},{"location":"operating_systems_%26_configs/graphics_drivers/#endeavour-os","title":"Endeavour OS","text":"<p>Endeavour OS has an installer on their repo (you won't find it on the Arch repos or AUR). You can install it with: <pre><code>sudo pacman -S nvidia-installer-dkms\n</code></pre></p> <p>You can then check if your card is supported by the tool: <pre><code>nvidia-installer-check\n</code></pre></p> <p>You will then be given a message letting you know if you can install the drivers.</p> <p>To install the drivers, type the command: <pre><code>sudo nvidia-installer-dkms\n</code></pre></p> <p>You will be prompted to reboot, and you should be good to go!</p>"},{"location":"operating_systems_%26_configs/graphics_drivers/#installing-cuda","title":"Installing CUDA","text":"<p>Installing CUDA on arch-based distros can be done simply by running the following command: <pre><code>sudo pacman -Sy cuda-tools\n</code></pre></p>"},{"location":"operating_systems_%26_configs/graphics_drivers/#ubuntu-install-rocm-drivers-for-radeon-graphics-cards","title":"Ubuntu - Install ROCm Drivers for Radeon Graphics Cards","text":"<p>NOTE: You cannot have ROCm and AMDGPU-Pro driver installed at the same time, if you already have AMDGPU-Pro installed, run the command:</p> <pre><code>amdgpu-pro-uninstall\n</code></pre> <ol> <li> <p>Add apt repo:    <pre><code>wget -qO - http://repo.radeon.com/rocm/apt/debian/rocm.gpg.key | sudo apt-key add - &amp;&amp; echo 'deb [arch=amd64] http://repo.radeon.com/rocm/apt/debian/ xenial main' | sudo tee /etc/apt/sources.list.d/rocm.listbash\n</code></pre></p> </li> <li> <p><code>bash    sudo apt update &amp;&amp; sudo apt install rocm-dkms</code></p> </li> <li> <p>To access the GPU, you must be a user in the video group. Ensure your user account is a member of the video group prior to using ROCm. To add your user to the video group, use the following command for the sudo password:    <pre><code>sudo usermod -a -G video $LOGNAME\n</code></pre></p> </li> <li> <p>By default, add any future users to the video group. Run the following command to add users to the video group:    <pre><code>echo 'ADD_EXTRA_GROUPS=1' | sudo tee -a /etc/adduser.conf\n\necho 'EXTRA_GROUPS=video' | sudo tee -a /etc/adduser.conf\n</code></pre></p> </li> <li> <p>Restart the system    <pre><code>reboot\n</code></pre></p> </li> <li> <p>After restarting the system, run the following commands to verify that the ROCm installation is successful. If you see your GPUs listed by both commands, the installation is considered successful.    <pre><code>/opt/rocm/bin/rocminfo\n\n/opt/rocm/opencl/bin/x86_64/clinfo\n</code></pre></p> </li> <li> <p>To run the ROCm programs more efficiently, add the ROCm binaries in your <code>PATH</code>.    <pre><code>echo 'export PATH=$PATH:/opt/rocm/bin:/opt/rocm/profiler/bin:/opt/rocm/opencl/bin/x86_64' | sudo tee -a /etc/profile.d/rocm.sh\n</code></pre></p> </li> </ol> <p>X. To remove ROCm: <pre><code>sudo apt autoremove rocm-dkms\n\nsudo apt autoremove rocm-libs miopen-hip cxlactivitylogger\n</code></pre></p>"},{"location":"operating_systems_%26_configs/install_ROCm_drivers/","title":"How to Install ROCm for AMD GPUs","text":"<p>Warning</p> <p>You cannot have ROCm and AMDGPU-Pro driver installed at the same time, if you already have AMDGPU-Pro installed, run the command: <pre><code>amdgpu-pro-uninstall\n</code></pre></p> <ol> <li> <p>Add apt repo:    <pre><code>wget -qO - http://repo.radeon.com/rocm/apt/debian/rocm.gpg.key | sudo apt-key add - &amp;&amp; echo 'deb [arch=amd64] http://repo.radeon.com/rocm/apt/debian/ xenial main' | sudo tee /etc/apt/sources.list.d/rocm.listbash\n</code></pre></p> </li> <li> <p><code>bash    sudo apt update &amp;&amp; sudo apt install rocm-dkms</code></p> </li> <li> <p>To access the GPU, you must be a user in the video group. Ensure your user account is a member of the video group prior to using ROCm. To add your user to the video group, use the following command for the sudo password:    <pre><code>sudo usermod -a -G video $LOGNAME\n</code></pre></p> </li> <li> <p>By default, add any future users to the video group. Run the following command to add users to the video group:    <pre><code>echo 'ADD_EXTRA_GROUPS=1' | sudo tee -a /etc/adduser.conf\n\necho 'EXTRA_GROUPS=video' | sudo tee -a /etc/adduser.conf\n</code></pre></p> </li> <li> <p>Restart the system    <pre><code>reboot\n</code></pre></p> </li> <li> <p>After restarting the system, run the following commands to verify that the ROCm installation is successful. If you see your GPUs listed by both commands, the installation is considered successful.    <pre><code>/opt/rocm/bin/rocminfo\n\n/opt/rocm/opencl/bin/x86_64/clinfo\n</code></pre></p> </li> <li> <p>To run the ROCm programs more efficiently, add the ROCm binaries in your <code>PATH</code>.    <pre><code>echo 'export PATH=$PATH:/opt/rocm/bin:/opt/rocm/profiler/bin:/opt/rocm/opencl/bin/x86_64' | sudo tee -a /etc/profile.d/rocm.sh\n</code></pre></p> </li> <li> <p>To remove ROCm:    <pre><code>sudo apt autoremove rocm-dkms\n\nsudo apt autoremove rocm-libs miopen-hip cxlactivitylogger\n</code></pre></p> </li> </ol>"},{"location":"operating_systems_%26_configs/install_arch/","title":"Install Arch Linux","text":"<p>Info</p> <p>This guide is based off of the DistroTube Arch Linux Installation Guide 2020</p> <p>Tip</p> <p>There is a new method for installing Arch linux using a script, while this is still in beta, they are slowly starting to roll it out to users. You can find more infor below:</p> <ul> <li>Installation video</li> <li>Arch Wiki Link</li> </ul>"},{"location":"operating_systems_%26_configs/install_arch/#base-system-install","title":"Base System Install","text":"<ol> <li> <p>Set keyboard layout:     <pre><code>loadkeys fr_CH\n</code></pre></p> </li> <li> <p>Check that internet services are working:     <pre><code>ping 1.1.1.1\n</code></pre></p> </li> <li> <p>Update the system clock:     <pre><code>timedatectl set-ntp true &amp;&amp; timedatectl status\n</code></pre></p> </li> <li> <p>Partition the disks</p> <ol> <li> <p>List the disks in your system:     <pre><code>fdisk -l\n</code></pre></p> </li> <li> <p>Open <code>fdisk</code> to create the partitions (in this example, we will assume that the disk is called <code>/dev/sda</code>)     <pre><code>fdisk /dev/sda\n</code></pre></p> </li> <li> <p>Inside <code>fdisk</code> - Create a new GPT partition table: Type <code>g</code>     We will create 3 new partitions, as shown in table below:</p> Partition # Type Size Partition 1 boot 550M Partition 2 swap 2G Partition 3 <code>/</code> (rest) <ol> <li> <p>Inside <code>fdisk</code> - Create Partition 1:</p> <ul> <li>Type <code>n</code> to create a new partition</li> <li>Type <code>1</code> to add a number to partition (<code>1</code> will also be chosen as default)</li> <li>Type <code>(Enter)</code> to select the default starting point for partition</li> <li>Type <code>+550M</code> to select the size of the partition</li> </ul> </li> <li> <p>Inside <code>fdisk</code> - Create Partition 2:</p> <ul> <li>Type <code>n</code> to create a new partition</li> <li>Type <code>2</code> to add a number to partition (<code>2</code> will also be chosen as default)</li> <li>Type <code>(Enter)</code> to select the default starting point for partition</li> <li>Type <code>+2G</code> to select the size of the partition</li> </ul> </li> <li> <p>Inside <code>fdisk</code> - Create Partition 3:</p> <ul> <li>Type <code>n</code> to create a new partition</li> <li>Type <code>3</code> to add a number to partition (<code>3</code> will also be chosen as default)</li> <li>Type <code>(Enter)</code> to select the default starting point for partition</li> <li>Type <code>(Enter)</code> to select the remaining disk space to be allocated to partition 3</li> </ul> </li> <li> <p>Inside <code>fdisk</code> - Select partition types</p> <ul> <li>Type <code>t</code> to start the partition type selection process</li> <li>Type <code>1</code> to select which partition we will set</li> <li>Type <code>1</code> to select the <code>EFI System</code> type</li> <li>Type <code>t</code> to start the partition type selection process</li> <li>Type <code>2</code> to select which partition we will set</li> <li>Type <code>19</code> to select the <code>Linux swap</code> type</li> </ul> <p>Partition 3 should be set as <code>Linux filesystem</code>, however as that is the default setting when the partition was created, we don't need to change it.</p> </li> <li> <p>Inside <code>fdisk</code> - Write the partitions to the disk</p> <ul> <li>Type <code>w</code> to write the partitions. This will also exit <code>fdisk</code> when executed</li> </ul> </li> </ol> </li> </ol> </li> <li> <p>Make the three filesystems for the three partitions that were created. Execute the following commands in order:     <pre><code>mkfs.fat -F32  /dev/sda1    # Make FAT32 filesystem for 1st partition\nmkswap /dev/sda2            # Make swap filesystem for 2nd partition\nswapon /dev/sda2            # Turn on swap filesystem for 2nd partition\nmkfs.ext4 /dev/sda3         # Make Ext4 filesystem for 3rd partition\n</code></pre></p> </li> <li> <p>Mount the Linux filesystem partition     <pre><code>mount /dev/sda3 /mnt\n</code></pre></p> </li> <li> <p>Install base system for Arch:     <pre><code>pacstrap /mnt base linux linux-firmware\n</code></pre></p> </li> <li> <p>Generate file system table:     <pre><code>genfstab -U /mnt &gt;&gt; /mnt/etc/fstab\n</code></pre></p> </li> <li> <p>Change into the root directory:     <pre><code>arch-chroot /mnt\n</code></pre></p> </li> <li> <p>Set the time-zone (we set the Zurich timezone here):     <pre><code>ln -sf /usr/share/zoneinfo/Europe/Zurich /etc/localtime &amp;&amp; hwclock --systohc\n</code></pre></p> <p>Tip</p> <p>You can list the regions by running: <code>ls /usr/share/zoneinfo/</code></p> </li> <li> <p>Set the locale</p> <ol> <li> <p>Start by installing the <code>vim</code> editor:     <pre><code>pacman -S vim\n</code></pre></p> </li> <li> <p>Edit the <code>/etc/locale.gen</code> <pre><code>vim /etc/locale.gen\n</code></pre></p> <ul> <li>Uncomment the line of your choice (line with <code>en_US.UTF-8 UTF-8</code> or line with <code>fr_CH.UTF-8 UTF-8</code>)</li> <li>Save and exit the file</li> </ul> </li> <li> <p>Run the command:     <pre><code>locale-gen\n</code></pre></p> </li> </ol> </li> <li> <p>Set the hostname</p> <ol> <li> <p>Create and edit the <code>/etc/hostname</code> file:     <pre><code>vim /etc/hostname\n</code></pre></p> </li> <li> <p>Write the hostname of your computer (this is the name of the computer)     <pre><code>&lt;hostname&gt;\n</code></pre></p> <p>Where <code>&lt;hostname&gt;</code> is the chosen hostname of the computer</p> </li> <li> <p>Edit the <code>/etc/hosts</code> file:     <pre><code>vim /etc/hosts\n</code></pre></p> <ul> <li>There should be a few lines commented in this file</li> <li>At the end of the file add the following     <pre><code>127.0.0.1       localhost\n::1             localhost\n127.0.1.1       &lt;hostname&gt;.localdomain      &lt;hostname&gt;\n</code></pre></li> <li>Save and exit the file</li> </ul> </li> </ol> </li> <li> <p>Create password for root:     <pre><code>passwd\n</code></pre></p> </li> <li> <p>Create non-root user and password for that user:     <pre><code>useradd -m &lt;username&gt; &amp;&amp; passwd &lt;password&gt;\n</code></pre></p> <p>Where  and  are the chosen username and password. <li> <p>Give non-root user sudo rights:     <pre><code>usermod -aG wheel,audio,video,storage,optical &lt;username&gt;\n</code></pre></p> </li> <li> <p>Install <code>sudo</code>:     <pre><code>pacman -S sudo\n</code></pre></p> </li> <li> <p>Allow members for the wheel group to access sudo</p> <ol> <li> <p>Open the <code>visudo</code> config file with <code>vim</code>:     <pre><code>EDITOR=vim visudo\n</code></pre></p> </li> <li> <p>Inside <code>visudo</code> - Uncomment the line about the wheen group (this will be under the <code>User priviledge specification</code> subtitle in the file, maybe line 82)</p> </li> <li>Exit and save the file</li> </ol> </li> <li> <p>Install grub:     <pre><code>pacman -S grub\n</code></pre></p> </li> <li> <p>Install EFI associate packages:     <pre><code>pacman -S efibootmgr dosfstools os-prober mtools\n</code></pre></p> </li> <li> <p>Make EFI directory and boot directory:     <pre><code>mkdir /boot/EFI\n</code></pre></p> </li> <li> <p>Mount partition:     <pre><code>mount /dev/sda1 /boot/EFI\n</code></pre></p> </li> <li> <p>Run grub-install command:     <pre><code>grub-install --target=x86_64-efi --bootloader-id=grub_uefi --recheck --no-nvram --removable\n</code></pre></p> </li> <li> <p>Generate grub config file:     <pre><code>grub-mkconfig -o /boot/grub/grub.cfg\n</code></pre></p> </li>"},{"location":"operating_systems_%26_configs/install_arch/#other-stuff-before-rebooting","title":"Other Stuff Before Rebooting","text":"<ol> <li> <p>Install network manager:     <pre><code>pacman -S networkmanager git\n</code></pre></p> </li> <li> <p>Enable network manager with systemd:     <pre><code>systemctl enable NetworkManager\n</code></pre></p> </li> <li> <p>Exit out of <code>chroot</code>:     <pre><code>exit\n</code></pre></p> </li> <li> <p>Unmount the <code>/mnt</code> directory:     <pre><code>umount -l /mnt\n</code></pre></p> </li> <li> <p>Shutdown the system (don't reboot this way you have time to remove installation media):     <pre><code>shutdown now\n</code></pre></p> </li> </ol>"},{"location":"operating_systems_%26_configs/install_arch/#install-aur-and-yay-utility","title":"Install AUR and <code>yay</code> Utility","text":"<ol> <li> <p>Instal makepkg capabilities:     <pre><code>sudo pacman -S base-devel\n</code></pre></p> </li> <li> <p>Install AUR:     <pre><code>git clone https://aur.archlinux.org/yay-git.git\n</code></pre></p> </li> <li> <p>Go into the <code>yay-git</code> directory and make the arch package:     <pre><code>cd yay-git\nmakepkg -si\n</code></pre></p> </li> </ol>"},{"location":"operating_systems_%26_configs/make_grub_menu_appear/","title":"Make Grub Menu Appear for Dual Boot Systems","text":"<p>Open the grub config file: </p> <pre><code>sudo vim /etc/default/grub\n</code></pre> <p>Change the lines:</p> <pre><code>GRUB_TIMEOUT_STYLE=hidden\nGRUB_TIMEOUT=0\n</code></pre> <p>to</p> <pre><code>GRUB_TIMEOUT_STYLE=menu\n#GRUB_TIMEOUT=0\n</code></pre> <p>Exit the file, then run the command</p> <pre><code>sudo update-grub\n</code></pre> <p>So that the changes can be registered. If you have a drive with another OS on it, it should be detected and displayed in the grub menu the next time you boot up. Good luck!</p>"},{"location":"operating_systems_%26_configs/make_windows_ubuntu_use_same_date_format/","title":"Make Windows &amp; Ubuntu Use the Same Date Format","text":"<p>To tell your Ubuntu system that the hardware clock is set to 'local' time, open a terminal and execute the following command:</p> <pre><code>timedatectl set-local-rtc 1\n</code></pre>"},{"location":"operating_systems_%26_configs/port_arch_linux_to_jetson/","title":"Port Arch Linux to Jetson","text":"<p>This guide will show you how to port Arch Linux onto the Nvidia Jetson Nano Devkit. Including installing the kernel, drivers, and configuration from the Linux for Nano release provided by NVIDIA. It will also provide steps for testing that the NVIDIA drivers have been installed correctly and are working correctly.</p>"},{"location":"operating_systems_%26_configs/port_arch_linux_to_jetson/#on-host-computer-preferably-running-linux","title":"On host computer (preferably running Linux)","text":""},{"location":"operating_systems_%26_configs/port_arch_linux_to_jetson/#obtaining-drivers-and-image","title":"Obtaining Drivers and Image","text":"<ol> <li> <p>Download the latest Nano Driver Package from https://developer.nvidia.com/linux-tegra     <pre><code>wget https://developer.nvidia.com/embedded/l4t/r32_release_v6.1/t210/jetson-210_linux_r32.6.1_aarch64.tbz2 Linux_for_Tegra.tbz2\n</code></pre></p> <p>Info</p> <p>The link is different for the AGX Xavier Jetson.</p> </li> <li> <p>Extract the downloaded Jetson Nano driver packages (from step 1) to a folder called <code>Linux_for_Tegra</code>:     <pre><code>sudo tar jxpf Linux_for_Tegra.tbz2\n</code></pre></p> </li> <li> <p>Go into the directories created from the extracted tar file (from step 2):     <pre><code>cd Linux_for_tegra  # We will refer to this directory as &lt;path_to_L4T_TOP_DIR&gt;\ncd rootfs           # We will refer to this directory as &lt;path_to_L4T_rootfs&gt;\n</code></pre></p> </li> <li> <p>Enter the <code>&lt;path_to_L4T_rootfs&gt;</code> folder, download and extract the arch image file:     <pre><code>cd rootfs\nwget http://os.archlinuxarm.org/os/ArchLinuxARM-aarch64-latest.tar.gz\nsudo tar -xpf ArchLinuxARM-aarch64-latest.tar.gz\n</code></pre></p> </li> </ol>"},{"location":"operating_systems_%26_configs/port_arch_linux_to_jetson/#changing-the-configuration-scripts","title":"Changing the Configuration Scripts","text":"<p>Compared to Ubuntu a few configuration changes are needed as Arch Linux has a different directory structure and init program. This section outlines the changes that need to be made to the archives in nv_tegra folder of the driver package of Release. Many of the files / directories need to be relocated as the file system hierarchy differs between Arch Linux and Ubuntu.</p> <ol> <li> <p>Modify the <code>nv_customize_rootfs.sh</code> <pre><code>sudo vim Linux_for_Tegra/nv_tools/scripts/nv_customize_rootfs.sh\n</code></pre></p> <p>Find the part of the script where: <pre><code>if [ -d \"${LDK_ROOTFS_DIR}/usr/lib/arm-linux-gnueabihf/tegra\" ]; then\nARM_ABI_DIR_ABS=\"usr/lib/arm-linux-gnueabihf\"\nelif [ -d \"${LDK_ROOTFS_DIR}/usr/lib/arm-linux-gnueabi/tegra\" ]; then\nARM_ABI_DIR_ABS=\"usr/lib/arm-linux-gnueabi\"\nelif [ -d \"${LDK_ROOTFS_DIR}/usr/lib/aarch64-linux-gnu/tegra\" ]; then\nARM_ABI_DIR_ABS=\"usr/lib/aarch64-linux-gnu\"\nelse\necho \"Error: None of Hardfp/Softfp Tegra libs found\"\nexit 4\nfi\n</code></pre></p> <p>And add the two lines: <pre><code>if [ -d \"${LDK_ROOTFS_DIR}/usr/lib/arm-linux-gnueabihf/tegra\" ]; then\nARM_ABI_DIR_ABS=\"usr/lib/arm-linux-gnueabihf\"\nelif [ -d \"${LDK_ROOTFS_DIR}/usr/lib/arm-linux-gnueabi/tegra\" ]; then\nARM_ABI_DIR_ABS=\"usr/lib/arm-linux-gnueabi\"\nelif [ -d \"${LDK_ROOTFS_DIR}/usr/lib/aarch64-linux-gnu/tegra\" ]; then\nARM_ABI_DIR_ABS=\"usr/lib/aarch64-linux-gnu\"\nelif [ -d \"${LDK_ROOTFS_DIR}/usr/lib/tegra\" ]; then                     # Add this line\nARM_ABI_DIR=\"${LDK_ROOTFS_DIR}/usr/lib\"                             # Add this line\nelse\necho \"Error: None of Hardfp/Softfp Tegra libs found\"\nexit 4\nfi\n</code></pre></p> </li> <li> <p>Extract the tars of the nv_tegra folder into directories of the same name:     <pre><code>cd Linux_for_Tegra/nv_tegra\nsudo mkdir nvidia_drivers config nv_tools nv_sample_apps/nvgstapps\n\nsudo tar -xpjf nvidia_drivers.tbz2 -C nvidia_drivers/\nsudo tar -xpjf config.tbz2 -C config/\nsudo tar -xpjf nv_tools.tbz2 -C nv_tools/\nsudo tar -xpjf nv_sample_apps/nvgstapps.tbz2 -C nv_sample_apps/nvgstapps/\n</code></pre></p> </li> </ol>"},{"location":"operating_systems_%26_configs/port_arch_linux_to_jetson/#changes-to-nvidia_drivers-package","title":"Changes to <code>nvidia_drivers</code> Package","text":"<ol> <li> <p>The <code>lib</code> folder will need to be moved into <code>/usr</code>:     <pre><code>cd Linux_for_Tegra/nv_tegra/nvidia_drivers\nsudo mv lib/* usr/lib/\nsudo rm -r lib\n</code></pre></p> </li> <li> <p>Everything in <code>usr/lib/aarch64-linux-gnu</code> will need to be moved into <code>usr/lib</code>:     <pre><code>sudo mv usr/lib/aarch64-linux-gnu/* usr/lib/\nsudo rm -r usr/lib/aarch64-linux-gnu\n</code></pre></p> </li> <li> <p><code>nv_tegra_release</code> in <code>etc/</code> will need to be updated to include the correct path to the tegra libraries:     <pre><code>sudo vim Linux_for_Tegra/nv_tegra/nvidia_drivers/etc/nv_tegra_release\n</code></pre></p> <p>The line in <code>nv_tegra_release</code> need to be replaced. Find the line: <pre><code>0c165125388fbd943e7f8b37a272dec7c5d57c15 */usr/lib/aarch64-linux-gnu/tegra/libnvmm.so\n</code></pre></p> <p>Replace it with the correct path: <pre><code>0c165125388fbd943e7f8b37a272dec7c5d57c15 */usr/lib/tegra/libnvmm.so\n</code></pre></p> </li> <li> <p>Now go into the folder <code>Linux_for_Tegra/nv_tegra/nvidia_drivers/etc/ld.so.conf.d/</code>:     <pre><code>sudo vim Linux_for_Tegra/nv_tegra/nvidia_drivers/etc/ld.so.conf.d/nvidia-tegra.conf\n</code></pre></p> <p>Point to the right directory and add the <code>tegra-egl</code> entry. The contents of <code>nvidia-tegra.conf</code> should look like this: <pre><code>/usr/lib/tegra\n/usr/lib/tegra-egl\n</code></pre></p> </li> </ol>"},{"location":"operating_systems_%26_configs/port_arch_linux_to_jetson/#changes-to-the-nv_tools-package","title":"Changes to the <code>nv_tools</code> Package","text":"<p>Note: In more recent versions of the nvidia drivers package (downloaded in step 1) you may not need to do step 11 as it has already done for you. Check the directories before running the commands in step 11.</p> <ol> <li>The tegrastats script should be moved from <code>home/ubuntu</code> into the <code>/usr/bin</code> directory. This removes the dependency on a user called <code>ubuntu</code>:     <pre><code>cd Linux_for_Tegra/nv_tegra/nv_tools\nmkdir -p usr/bin\nmv home/ubuntu/tegrastats usr/bin/\nrm -r home\n</code></pre></li> </ol>"},{"location":"operating_systems_%26_configs/port_arch_linux_to_jetson/#changes-to-nvgstapps-package","title":"Changes to <code>nvgstapps</code> Package","text":"<p>Note: In more recent versions of the nvidia drivers package (downloaded in step 1) you may not need to do step 12 as it has already done for you. Check the directories before running the commands in step 12.</p> <ol> <li> <p>Move the contents of <code>usr/sbin</code> into `usr/bin``     <pre><code>cd Linux_for_Tegra/nv_tegra/nv_sample_apps/nvgstapps/\nsudo mv usr/sbin/* usr/bin\nsudo rm -r usr/sbin\n</code></pre></p> </li> <li> <p>Move the contents of <code>usr/lib/aarch64-linux-gnu</code> to <code>usr/lib</code> <pre><code>sudo mv usr/lib/aarch64-linux-gnu/* usr/lib/\nsudo rm -r usr/lib/aarch64-linux-gnu\n</code></pre></p> </li> </ol>"},{"location":"operating_systems_%26_configs/port_arch_linux_to_jetson/#finalizing-configuration-changes","title":"Finalizing Configuration Changes","text":"<ol> <li> <p>When you have finished making all the listed changes, repackage the files:     <pre><code>cd &lt;path_to_L4T_TOP_DIR&gt;/nv_tegra\n</code></pre></p> <pre><code>cd nvidia_drivers\nsudo tar -cpjf ../nvidia_drivers.tbz2 *\ncd ..\n</code></pre> <pre><code>cd config\nsudo tar -cpjf ../config.tbz2 *\ncd ..\n</code></pre> <pre><code>cd nv_tools\nsudo tar -cpjf ../nv_tools.tbz2 *\ncd ..\n</code></pre> <pre><code>cd nv_sample_apps/nvgstapps\nsudo tar -cpjf ../nvgstapps.tbz2 *\ncd ../..\n</code></pre> </li> </ol>"},{"location":"operating_systems_%26_configs/port_arch_linux_to_jetson/#changes-to-rootfs","title":"Changes to <code>rootfs</code>","text":"<p>The following are changes that will be made to contents in your rootfs directory.</p>"},{"location":"operating_systems_%26_configs/port_arch_linux_to_jetson/#initialization-script","title":"Initialization Script\u200b","text":"<p>As Arch Linux uses systemd rather than upstart, the init script will need to be converted into a systemd service. Information on systemd and how to create services can be found on the Arch Linux Wiki page for systemd</p> <ol> <li> <p>To create the systemd service, we will need the service descriptor file, that tells systemd about the service. Hence need to create a service file as below in <code>&lt;path_to_L4T_TOP_DIR&gt;/rootfs/usr/lib/systemd/system/nvidia-tegra.service</code>:</p> <p>Tip</p> <p>Path may actually be <code>&lt;path_to_L4T_TOP_DIR&gt;/usr/lib/systemd/system/nvidia-tegra.service</code>. Try the first one before showing this guide to anyone else!!</p> <p>Create and open file in vim editor: <pre><code>sudo vim Linux_for_Tegra/usr/lib/systemd/system/nvidia-tegra.service\n</code></pre></p> <p>File contents: <pre><code>##Location\n## /usr/lib/systemd/system/nvidia-tegra.service\n[Unit]\nDescription=The NVIDIA tegra init script\n\n[Service]\ntype=oneshot\nRemainAfterExit=yes\nExecStart=/usr/bin/nvidia-tegra-init-script\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></p> </li> <li> <p>Similarly, create a new file at <code>&lt;path_to_L4T_TOP_DIR&gt;/rootfs/usr/bin/nvidia-tegra-init-script</code>, and place these contents inside:</p> <p>Create and open fil in vim editor: <pre><code>sudo vim Linux_for_Tegra/rootfs/usr/bin/nvidia-tegra-init-script\n</code></pre></p> <p>File contents: <pre><code>#!/bin/bash\nif [ -e /sys/power/state ]; then\nchmod 0666 /sys/power/state\nfi\nif [ -e /sys/devices/soc0/family ]; then\nSOCFAMILY=\"`cat /sys/devices/soc0/family`\"\nfi\nif [ \"$SOCFAMILY\" = \"Tegra210\" ] &amp;&amp;\n[ -e /sys/devices/system/cpu/cpu0/cpufreq/scaling_min_freq ]; then\nsudo bash -c \"echo -n 510000 &gt; /sys/devices/system/cpu/cpu0/cpufreq/scaling_min_freq\"\nfi\nif [ -d /sys/devices/system/cpu/cpuquiet/tegra_cpuquiet ] ; then\necho 500 &gt; /sys/devices/system/cpu/cpuquiet/tegra_cpuquiet/down_delay\n    echo 1 &gt; /sys/devices/system/cpu/cpuquiet/tegra_cpuquiet/enable\nelif [ -w /sys/module/cpu_tegra210/parameters/auto_hotplug ] ; then\necho 1 &gt; /sys/module/cpu_tegra210/parameters/auto_hotplug\nfi\nif [ -e /sys/module/cpuidle/parameters/power_down_in_idle ] ; then\necho \"Y\" &gt; /sys/module/cpuidle/parameters/power_down_in_idle\nelif [ -e /sys/module/cpuidle/parameters/lp2_in_idle ] ; then\necho \"Y\" &gt; /sys/module/cpuidle/parameters/lp2_in_idle\nfi\nif [ -e /sys/block/sda0/queue/read_ahead_kb ]; then\necho 2048 &gt; /sys/block/sda0/queue/read_ahead_kb\nfi\nif [ -e /sys/block/sda1/queue/read_ahead_kb ]; then\necho 2048 &gt; /sys/block/sda1/queue/read_ahead_kb\nfi\nfor uartInst in 0 1 2 3\ndo\nuartNode=\"/dev/ttyHS$uartInst\"\nif [ -e \"$uartNode\" ]; then\nln -s /dev/ttyHS$uartInst /dev/ttyTHS$uartInst\nfi\ndone\nmachine=`cat /sys/devices/soc0/machine`\nif [ \"${machine}\" = \"jetson-nano-devkit\" ] ; then\necho 4 &gt; /sys/class/graphics/fb0/blank\n            BoardRevision=`cat /proc/device-tree/chosen/board_info/major_revision`\nif [ \"${BoardRevision}\" = \"A\" ] ||\n[ \"${BoardRevision}\" = \"B\" ] ||\n[ \"${BoardRevision}\" = \"C\" ] ||\n[ \"${BoardRevision}\" = \"D\" ]; then\necho 0 &gt; /sys/devices/platform/tegra-otg/enable_device\n                    echo 1 &gt; /sys/devices/platform/tegra-otg/enable_host\n            fi\nfi\nif [ -e /sys/devices/system/cpu/cpu0/cpufreq/scaling_available_governors ]; then\nread governors &lt; /sys/devices/system/cpu/cpu0/cpufreq/scaling_available_governors\n    case $governors in\n*interactive*)\necho interactive &gt; /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor\n            if [ -e /sys/devices/system/cpu/cpufreq/interactive ] ; then\necho \"1224000\" &gt;/sys/devices/system/cpu/cpufreq/interactive/hispeed_freq\n                echo \"95\" &gt;/sys/devices/system/cpu/cpufreq/interactive/target_loads\n                echo \"20000\" &gt;/sys/devices/system/cpu/cpufreq/interactive/min_sample_time\n            fi\n;;\n*)\n;;\nesac\nfi\necho \"Success! Exiting\"\nexit 0\n</code></pre></p> </li> <li> <p>Enable the script before flashing / first boot by create the following symbolic link to enable the service:     <pre><code>cd Linux_for_Tegra/rootfs/etc/systemd/system/sysinit.target.wants/\nln -s /usr/lib/systemd/system/nvidia-tegra.service nvidia-tegra.service\n</code></pre></p> <p>This should be executed after apply_binaries, so the nvidia-tegra service is in place.</p> </li> </ol>"},{"location":"operating_systems_%26_configs/port_arch_linux_to_jetson/#pacman-configuration","title":"Pacman Configuration","text":"<ol> <li> <p>As we have installed a custom kernel to boot linux on the jetson-nano-devkit, it is necessary to update pacman.conf to ignore updates to the kernel package. To do so add <code>linux</code> as an ignored package to your <code>&lt;path_to_L4T_rootfs&gt;/etc/pacman.conf</code> as below:</p> <p>Open file in vim editor: <pre><code>sudo vim Linux_for_Tegra/rootfs/etc/pacman.conf\n</code></pre></p> <p>Edit the following line to: <pre><code>IgnorePkg=linux\n</code></pre></p> </li> </ol>"},{"location":"operating_systems_%26_configs/port_arch_linux_to_jetson/#flashing-the-jetson","title":"Flashing the Jetson","text":"<p>The steps for flashing the Arch Linux image to the Jetson are no different than flashing the image for Ubuntu.</p> <ol> <li> <p>Enable recovery mode on the Jetson:</p> <ul> <li> <p>Enabling recovery Mode on the Jetson:</p> <ol> <li>Jumper the J48 power select pin first and plug the power jack</li> <li>Jumper the recovery pin</li> <li>Jumper the reset pin</li> <li>Remove the jumper of reset pin</li> <li>Remove the jumper of recovery pin.</li> </ol> <p>You should be able to see an nvidia device with <code>lsusb</code> command on your host computer.</p> </li> </ul> </li> <li> <p>Plug the jetson in via USB to the host computer (where you have done steps 1-18).</p> </li> <li> <p>Apply the NVIDIA specific configuration, binaries and the L4T kernel:     <pre><code>sudo ./apply_binaries.sh\n</code></pre></p> </li> <li> <p>Create the image from the <code>rootfs</code> directory and flash the image to the Jetson:     <pre><code>sudo ./flash.sh jetson-nano-devkit mmcblk0p1\n</code></pre></p> <p>Tip</p> <p>You may need to change the big <code>if</code> statement of the <code>apply_binaries.sh</code> script to get it to apply the changes you made in the previous steps. Test and see.</p> </li> <li> <p>Your device should reboot and prompt you to login. The default login for Arch Linux ARM is:     Username: root     Password: root</p> </li> </ol>"},{"location":"operating_systems_%26_configs/raspberry_pi_camera/","title":"Get Camera Working on Raspberry Pi","text":"<p>The built-in MIPI connector for cameras on the raspberry pi might give users some greif, os this is a short guide of things to try.</p>"},{"location":"operating_systems_%26_configs/raspberry_pi_camera/#add-user-to-video-group","title":"Add user to video group","text":"<p>This is really simple and stupid, but it's not enabled by default. Enter in the command below and then reboot for the changes to take effect:</p> <pre><code>sudo usermod -a -G video $USER\n</code></pre>"},{"location":"operating_systems_%26_configs/raspberry_pi_camera/#video-drivers","title":"Video Drivers","text":"<p>To ensure that you have the correct video drivers for linux, run the following command:</p> <pre><code>sudo modprobe bcm2835-v4l2 </code></pre>"},{"location":"operating_systems_%26_configs/raspberry_pi_camera/#install-dietpi","title":"Install DietPi","text":"<p>Dietpi is essentially a stripped down and modded version of Raspbian, but it's performance has been proven to be quite good after personal use.</p> <p>Link: https://dietpi.com</p>"},{"location":"operating_systems_%26_configs/show_battery_percentage_level_on_XFCE/","title":"Show Battery Percentage Level on XFCE","text":"<p>Open terminal, and install:</p> <pre><code>sudo apt-get update &amp;&amp; sudo apt-get install xfce4-battery-plugin\n</code></pre> <p>Then add the new plugin to the panel. To configure what to display in the battery indicator:</p> <ol> <li>Right click on the battery bar icon of battery monitor plugin</li> <li>Click on Properties menu</li> <li>This will display the Battery Monitor Properties dialog box. From there, you can configure:<ul> <li>Different actions to take on different battery level</li> <li>Colors of the battery bar to be displayed</li> <li>Options to display different things like percentage, time, bar, etc.</li> </ul> </li> </ol>"},{"location":"operating_systems_%26_configs/ssh_error/","title":"SSH Errors &amp; How to Fix Them","text":""},{"location":"operating_systems_%26_configs/ssh_error/#remote-host-id-has-changed","title":"Remote Host ID has Changed","text":"<p>If you used <code>ssh</code> command to connect to the remote server, but you got the following error message causing you can not connect:</p> <pre><code>@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\n@    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @\n@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@\nIT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!\nSomeone could be eavesdropping on you right now (man-in-the-middle attack)!\nIt is also possible that a host key has just been changed.\nThe fingerprint for the ECDSA key sent by the remote host is\nSHA256:wruvASsAzradn/Acy0g0YwuDGBZb7ierhmL/fhhsSu4.\nPlease contact your system administrator.\nAdd correct host key in /Users/clay/.ssh/known_hosts to get rid of this message.\nOffending ECDSA key in /Users/clay/.ssh/known_hosts:13\nECDSA host key for ohio.cs.nccu.edu.tw has changed and you have requested strict checking.\nHost key verification failed.\n</code></pre> <p>The solution is to run the following command: <pre><code>ssh-keygen -R &lt;ip-address&gt;\n</code></pre></p> <p>Where <code>&lt;ip-address&gt;</code> is the IP of the remote connection you were attemptingto connect to. Re-run the standard SSH command and you should be good to go!</p>"},{"location":"operating_systems_%26_configs/stop_ds_store_file_creation/","title":"Stop macOS from Creating <code>.DS_Store</code> Files","text":""},{"location":"operating_systems_%26_configs/stop_ds_store_file_creation/#introduction","title":"Introduction","text":"<p>macOS creates a <code>.DS_Store</code> file in every folder you view in Finder. This file stores metadata about that folder\u2019s contents as well as user customizations for things like view type and icon size.</p> <p>These <code>.DS_Store</code> files are hidden from you in macOS so they won\u2019t clutter up your folder views. But in mixed-OS environments, the <code>.DS_Store</code> files can become a problem. That\u2019s because your Mac creates these files even for shared network locations. So if you\u2019re sharing a NAS at your office with people using Windows PCs, they may suddenly see a bunch of <code>.DS_Store</code> files littering the shared directories.</p>"},{"location":"operating_systems_%26_configs/stop_ds_store_file_creation/#prevent-ds_store-creation","title":"Prevent <code>.DS_Store</code> Creation","text":"<p>To configure your Mac to not create <code>.DS_Store</code> files on shared network drives, log into macOS, launch the Terminal, and enter the following command:</p> <pre><code>defaults write com.apple.desktopservices DSDontWriteNetworkStores -bool TRUE\n</code></pre> <p>Once you\u2019ve executed the command, save any open work and log out of your macOS user account. When you log back in, reconnect to your shared network drives. Existing <code>.DS_Store</code> files may still be present and will need to be manually deleted, but your Mac won\u2019t create any new <code>.DS_Store</code> files as you browse the shared directories going forward.</p>"},{"location":"operating_systems_%26_configs/stop_ds_store_file_creation/#re-enable-ds_store-creation","title":"Re-Enable <code>.DS_Store</code> Creation","text":"<p>If you\u2019ve used the command above to disable the creation of <code>.DS_Store</code> files on shared network drives, you can re-enable the creation of these files with the following command:</p> <pre><code>defaults write com.apple.desktopservices DSDontWriteNetworkStores -bool FALSE\n</code></pre> <p>As before, make sure to log out and then reconnect your shared network drives after running the command.</p>"},{"location":"operating_systems_%26_configs/ubuntu_fix_error_detected_message/","title":"How To Fix \"System Program Problem Detected In Ubuntu\"","text":"<ol> <li> <p>Open a terminal and use the following command:    <pre><code>sudo rm /var/crash/*\n</code></pre>    This will delete all the content of directory <code>/var/crash</code>. This way you won\u2019t be annoyed by the pop up for the programs crash that happened in the past.</p> </li> <li> <p>Permanently get rid of system error pop up in Ubuntu. To disable the apport and get rid of system crash report completely, open a terminal and use the following command to edit the apport settings file:    <pre><code>sudo gedit /etc/default/apport &amp;\n</code></pre></p> </li> <li> <p>The content of the file is:    <pre><code># set this to 0 to disable apport, or to 1 to enable it\n# you can temporarily override this with\n# sudo service apport start force_start=1\nenabled=1\n</code></pre></p> </li> </ol> <p>Change the <code>enabled=1</code> to <code>enabled=0</code>. Save and close the file.</p>"},{"location":"operating_systems_%26_configs/ubuntu_stop_automatically_adding_printers/","title":"Stop Ubuntu from Adding All Printers on Network","text":"<p>Open terminal and type in command: <pre><code>sudo systemctl stop cups-browsed\n</code></pre></p> <p>followed by: <pre><code>sudo systemctl disable cups-browsed\n</code></pre></p> <p>You may still start/stop the service manually if you want: <pre><code>sudo systemctl start cups-browsed\nsudo systemctl stop cups-browsed\n</code></pre></p>"},{"location":"operating_systems_%26_configs/ubuntu_stop_screen_tearing/","title":"Stop Screen Tearing on Ubuntu","text":"Intel GraphicsNVIDIA GraphicsAMD Graphics <p>Intel graphics on Linux usually aren\u2019t too much of a problem. That\u2019s probably because integrated graphics usually have fewer features, and the Intel driver stack is mostly open source. For screen tearing on Intel, the solution usually comes in the form of some additional configuration.</p> <p>Because Intel uses open source drivers, the Xorg configuration is going to be your most direct route. Create a file at <code>/etc/X11/xorg.conf.d/20-intel.conf</code> or <code>/usr/share/X11/xorg.conf/20-intel.conf</code>, then place the following block of code inside:</p> <pre><code>Section \"Device\"\nIdentifier \"Intel Graphics\"\nDriver \"intel\"\nOption \"TearFree\" \"true\"\nEndSection\n</code></pre> <p>When you\u2019re done, save and reboot.</p> <pre><code>reboot\n</code></pre> <ol> <li>Use the NVIDIA proprietary driver</li> <li>Open the <code>NVIDIA X Server Settings</code> app</li> <li>Select the <code>X Server Display Configuration</code></li> <li>Click on the <code>Advanced</code> button and check <code>Force Composition Pipeline</code> option</li> <li>Click on the button <code>Save to X Configuration file</code> to set it at startup</li> </ol> <p>Tip</p> <p>Fix For Firefox 1. Go to <code>about:config</code> 2. Select <code>layers.acceleration.force-enabled</code> and set to <code>true</code></p> <ol> <li> <p>Create a new configuration directory <code>/etc/X11/xorg.conf.d/</code> <pre><code>sudo mkdir -p /etc/X11/xorg.conf.d/\n</code></pre></p> </li> <li> <p>Use your favorite text editor to create a configuration file called <code>20-radeon.conf</code>: <pre><code>sudo vim /etc/X11/xorg.conf.d/20-radeon.conf\n</code></pre></p> </li> <li> <p>Put the information below inside the <code>20-radeon.conf</code> file and then save and exit the file: <pre><code>Section \"Device\"\nIdentifier \"Radeon\"\nDriver \"radeon\"\nOption \"TearFree\" \"on\"\nEndSection\n</code></pre></p> </li> <li> <p>Reboot your system for the new configuration to take effect. <pre><code>reboot\n</code></pre></p> </li> </ol> <p>X. If you still notice some screen tearing after preforming the steps above, then you'll need to edit the <code>20-radeon.conf</code> file to include an extra option. Use your text editor to edit the <code>20-radeon.conf</code> file to include the <code>AccelMethod</code> and <code>DRI</code> options and then reboot your system. <pre><code>sudo vim /etc/X11/xorg.conf.d/20-radeon.conf\n</code></pre></p> <pre><code>Section \"Device\"\nIdentifier \"Radeon\"\nDriver \"radeon\"\nOption \"TearFree\" \"on\"\nOption \"DRI\" \"3\"\nOption \"AccelMethod\" \"glamor\"\nEndSection\n</code></pre> <pre><code>reboot\n</code></pre>"},{"location":"operating_systems_%26_configs/ubuntu_working_with_external_drives/","title":"Working with External Drives on Ubuntu","text":"<ol> <li> <p>To see all drives, you can type in the command:     <pre><code>df -h\n</code></pre></p> </li> <li> <p>If you want to format a drive, you can type in the command:     <pre><code>sudo umount /dev/sdXX\n</code></pre></p> <p>Where <code>/dev/sdXX/</code> is the name of the drive you can find using the command <code>df -h</code>.</p> <p>You must then create a file system in the drive, and you can do this with the <code>mkfs</code> command.</p> </li> </ol>"},{"location":"operating_systems_%26_configs/wifi_drivers_arch_linux/","title":"Get Wifi Drivers on Mac Hardware for Arch Linux","text":"<p>The problem is that, by default, Linux uses the BCMA driver. Even if you install the <code>broadcom-wl</code> driver, it doesn\u2019t use it by default. Therefore you need to do the following:</p> <ol> <li> <p>Install the correct <code>broadcom-wl</code> driver. <code>broadcom-wl-dkms</code> did not work for me but you could try it. <code>broadcom-wl</code> is kernel dependent so make sure you install the correct one.</p> </li> <li> <p>Blacklist the BCMA driver in <code>/etc/modprobe.d/blacklist.conf</code> adding the text \"blacklist bcma\".</p> </li> <li> <p>Install <code>broadcom-wl</code>. To do this, type <code>sudo pacman -S broadcom-wl</code> in the terminal. It will show a few options. The numbers you see before broadcom-wl are kernel versions. Find your kernel version using <code>uname -srm</code> and use the version for your kernel (e.g kernel 5.10.x uses 510, etc.).</p> </li> </ol>"},{"location":"other/add_ssh_key_to_git_repo/","title":"How to add SSH key to Git Repo","text":"<ol> <li> <p>Set up your default identity. In terminal, write the command:     <pre><code>ssh-keygen\n</code></pre></p> </li> <li> <p>List the contents of <code>~/.ssh</code> to view the key files.     <pre><code>ls ~/.ssh\n</code></pre></p> </li> <li> <p>Add the key to the ssh-agent. To start the agent, run the following:     <pre><code>eval `ssh-agent`\n</code></pre></p> </li> <li> <p>Add the public key to your online git settings. Open the account settings page, then find and go to the SSH Keys settings. You will be prompted to add a key to your account.</p> </li> <li> <p>Retreive the public key that you generated</p> LinuxmacOS <p>On Linux, you can cat the contents: <pre><code>cat ~/.ssh/id_rsa.pub\n</code></pre></p> <p>On macOS, the following command copies the output to the clipboard: <pre><code>pbcopy &lt; ~/.ssh/id_rsa.pub\n</code></pre></p> </li> <li> <p>Select and copy the key output in the clipboard. Paste in the browser and click save.</p> </li> </ol>"},{"location":"other/compress_pdf_in_terminal/","title":"How to Compress a PDF in Terminal","text":"<p>Open the terminal and go to the directory where the original PDF is located. For this example, let us say that the name of the PDF in question is called <code>original.pdf</code>.</p> <p>Type in the command:</p> <pre><code>gs -sDEVICE=pdfwrite -dCompatibilityLevel=1.4 -dPDFSETTINGS=/prepress -dNOPAUSE -dQUIET -dBATCH -sOutputFile=original_compressed.pdf original.pdf\n</code></pre> <p>And you should have the new file called <code>original_compressed.pdf</code> in the same directory!</p>"},{"location":"other/loop_through_files/","title":"Looping through Files and Performing Tasks","text":"<p>To be able to easily perform a command onto many files in a folder, you can use a simple shell <code>for</code> loop: <pre><code>for f in *.png; do convert $f -trim +repage $f; done\n</code></pre></p> <p>Remeber to put a semi-colon between each new line!</p>"},{"location":"other/ms_office_templates/","title":"MS Office Templates","text":"<p>This is a quick guide to help create and use templates in MS Office.</p>"},{"location":"other/ms_office_templates/#creating-a-template","title":"Creating a Template","text":"<p>You can start to create a template by going opening the office app and heading to \"View \u2192 Slide Master\". You will then be able to see all of the slides that make up the template. You can edit them and rename the per-slide templates.</p> <p>When finished creating the template, you can save it by doing the following:</p> WindowsmacOS <p>Go to \"File \u2192 Save\", and save file with the <code>.potx</code> extension to the desired location.</p> <p>Go to \"File \u2192 Save as Template...\" and save the template to the desired location.</p>"},{"location":"other/ms_office_templates/#set-template-location","title":"Set Template Location","text":"<p>Correctly setting the template location is what will determine whether your teamplate shows up in the MS Office app or not.</p> WindowsmacOS <p>On Windows you can set the desired location of your personal templates by going to \"File \u2192 Options \u2192 Save \u2192 Default personal templates location\", and setting the path there.</p> <p>On macOS you unfortunately cannot set the location of the personal templates, the path is fixed at: <code>~/Library/Group Containers/UBF8T346G9.Office/User Content/Templates</code>.</p> <p>Note that this path is hidden by default, but you can see hidden files in Finder by pressing Cmd+Shift+. on the keyboard.</p>"},{"location":"other/solar_arrays/","title":"Solar Arrays and Custom Installation","text":"<p>Info</p> <p>This guide was made as an accumulation of the video guides listed below:</p> <ul> <li>DIY Solar System vs Commercial Offer</li> <li>Do It Yourself Solar Power? - Easy DIY Solar Panel Installation!</li> </ul>"},{"location":"other/solar_arrays/#introduction","title":"Introduction","text":"<p>Recently, energy has become a more critical and therefore expensive resource. The ability to produce high-quality electrical energy is becoming increasingly important for countries, as so many people rely on this resource. Without it, accomplishing simple tasks, such as heating a room, keeping food cold, or communicating with others, become more tedious and inefficient. While many methods have been explored, renewable methods provide a way to produce electrical energy whilst having a minimal impact on the surrounding environment.</p> <p>The most common and inexpensive renewable method for an individual to produce electrical energy is to use solar arrays. Adding a solar array to one's rooftop is becoming increasingly popular in an attempt to alleviate the additional costs that are brought on by energy companies year after year, and as their popularity grows, the more their price decreases. Solar arrays have also been shown to be capable of operating at 80% efficiency for 25-30 years, and still producing energy after that period.</p> <p>In this guide we will go over the different methods for solar panel mounting as well as the techincal reasoning behind them.</p>"},{"location":"other/solar_arrays/#solar-array-system","title":"Solar Array System","text":"<p>A general solar array system is composed of two or three components:</p> <ul> <li>Solar Array</li> <li>Inverter</li> <li>Battery (optional for a grid-connected system)</li> </ul> <p>This setup can then be connected to the grid, or remain as an off-grid system. A simple representation of such a system is shown below:</p> <p> </p> <p>With a grid-connected system, it is possible to provide extra power back into the grid, therefore 'selling' your power back to the electrical company for a price. Depending on the installed solar array and your location, it is possible to receive more that 1'000 CHF per year with a grid-connected system.</p> <p>An off-grid system is not connected to the power-grid (as its name suggests), thus making the battery required to store any collected energy.</p>"},{"location":"other/solar_arrays/#installation-possibilities","title":"Installation Possibilities","text":"<p>As solar arrays are becoming more common, many different installation methods are becoming available on the market. It is even possible to buy a kit and mount them yourself. We will also go over how a DIY setup compares with the more common methods for installation.</p>"},{"location":"other/solar_arrays/#method-for-roof-installation","title":"Method for Roof Installation","text":"<p>It is obviously preferable to avoid large modifications to the building when installing solar arrays, as such installations can increase costs drastically, as well as delay the entire process. It is also important to use proper materials that will survive different weather conditions that they are subjected to over the lifespan of the array.</p> <p>A newer method from a company K2 Systems uses simple weights placed onto aluminium extrusions. This makes the installation easy, as no holes need to be drilled into the roof, and reliable, as the aliminium extrusions are well suited to extreme weather conditions. The solar array are therefore laid onto the roof, making it easy to replace or change the arrangement if needed in the future.</p>"},{"location":"other/solar_arrays/#solar-panel-mounting-optimization","title":"Solar Panel Mounting Optimization","text":"<p>There are two common mounting options when mounting panels onto your roof. The first is south-facing panel (for those who live in the northern hemisphere), and the second is east-west facing panels. It is important to know which arrangement is being recommended, as this will greatly influence the ability for the system to produce maximum power. Let's look at a side-view of both systems:</p> <p> </p> <p>We can see in the figure above, that the south orienting panels have gaps in between the panels, as shade is created by the panel in front. In the east-west configuration, the only gap between the panels is to allow for maintenance space.</p> <p>There is an online calculator tool provided by the European Commission where you can calculate the optimal exposure to the sun: https://re.jrc.ec.europa.eu/pvg_tools/en/tools.html</p> <p>You can input your location in Europe and it can optimize the slope and azimuth angle of the panels for you. In previous years, with high-priced panels, the south-facing configuration was preferred to save on cost of the panels, but as solar panels have decreased in price, optimizing for the roof performance is preferable tha optimizing for panel performance.</p> <p>Then why is the south-facing configuration still offered as an option? This is most likely due to the seeling price vs. the buying price of energy that is established by the electrical company. For optimum payback, it is therefore better to have a smaller panel area.</p> <p>One can also optimize when the energy is returned to the grid, as buying electricity during the day is more expensive that buying electrical energy at night. However to implement this, a battery becomes a requirement.</p>"},{"location":"other/solar_arrays/#inverter-battery","title":"Inverter &amp; Battery","text":"<p>The current output of solar panels is DC (direct current), an inverter is required to transform the direct current (DC) to alternating current (AC). This alternating current must be in phase and at the same frequency as the grid frequency if the inverter is connected to the grid.</p> <p>To construct an off-grid system any inverter can be used, however for a grid-connected system, the inverter needs to be approved by the electricity company.</p>"},{"location":"other/solar_arrays/#tasks-of-the-inverter","title":"Tasks of the Inverter","text":"<p>The inverter has a large number of tasks to accomplish, they are listed below: </p> <ul> <li>Synchronize its frequency and phase before joining the grid network</li> <li>Manage the current inserted into the network according to the sun</li> <li>Battery management</li> <li>Switch to an off-grid scenario during a power outage</li> <li>Switch back if power comes on after a power outage</li> <li>Provide system information to home automation system</li> </ul> <p>This is why it is important to buy a high quality inverter from a reputable brand. It is also important that the inverter incorporates several EMi and current filters, as performing such tasks incorrectly can lead to a lot of radio interference and inconsistent output.</p> <p>Warning</p> <p>Pay attention when you select your inverter! Not all provide off-grid functionality. Most of them provide this functionality only for one phase!</p>"},{"location":"other/solar_arrays/#panel-installation-and-power-routing","title":"Panel Installation and Power Routing","text":"<p>In a common installation, the panels next to each other are connected in series and therefore have to carry the same current. But happens if one of these panels is put into shade by a chimney, or other object? With the panels connected in series, even if one panel gives less current, it will affect all of the other panels connected to it in series.</p>"},{"location":"other/solar_arrays/#bypass-diodes","title":"Bypass Diodes","text":"<p>One common solution to such a problem is to use bypass diodes, that allow the current to pass in such a situation (see figure below). With this setup, the panel in the shade will not provide energy, but will not prohibit the current from the other panels in the string.</p> <p></p>"},{"location":"other/solar_arrays/#optimizers","title":"Optimizers","text":"<p>Another newer solution is to use DC-DC converters, known as \"optimizers\". These converters are attached to each solar panel, and align the current of each panel to the string current by reducing the voltage. This way, the performance of a shaded panel can used to its maximum potential.</p> <p>This at first sounds like a good solution, however there are common caveats with optimizers that are important to note:</p> <ul> <li>They have to operate in a hot environment (directly below the panels), so their life-span is short</li> <li>If an optimizer is installed for each panel, the risk of an early death due to manufacturing errors increases</li> <li>Given their location, optimizers might not be easy to replace depending on the installation situation on the roof</li> <li>If they are made cheaply, they can create a lot of radio interference</li> </ul>"},{"location":"other/solar_arrays/#micro-inverters","title":"Micro-Inverters","text":"<p>The final explored solution is to use micro-inverters. As was mentioned previously, we would normally need a large inverter to handle a variety of tasks. The most important task of an inverter is to convert the high DC voltage to a reasonable AC voltage for daily use (240 VAC).</p> <p>However it is becoming more popular to divide this task up throughout many smaller inverters located in proximity to the panel. This way, AC current is generated on the roof, and even when a group of panels is in the shade, no energy is lost in the system. Furthermore, as good quality inverters can have a very long lifetime, a lot of the caveats found with optimizers do not apply to micro-inverters.</p>"},{"location":"other/solar_arrays/#battery","title":"Battery","text":"<p>As we saw before, the battery for such a system is optional depending on what kind of system you want to install. While a battery can be made use of during a power outage, it is worth factoring into the cost calculation as batteries are becoming more expensive.</p> <p>Certain types of batteries are also known to be a fire hazard. Depending on their chemistry, it can be extremely difficult to put out a fire created by some batteries. A good battery chemistry to use to avoid such a risk would be a \\(\\text{LiFePo}_4\\) battery.</p>"},{"location":"other/ssh_raspi_usb/","title":"SSH into Raspberry Pi through USB","text":"<ol> <li> <p>Edit the image: To access the Pi Zero over USB you have to edit the image first.</p> <ul> <li>If you have the SD card in your Pi Zero, power it down and remove it</li> <li>Put the SD card in an adapter and plug it into your computer</li> <li>On a Mac the SD card should appear on your desktop</li> <li>Open the SD card icon to explore the contents</li> </ul> </li> <li> <p>Access the micro SD card from the command line.</p> <p>In the command line enter the following command:</p> <pre><code>ls -ls /Volumes/\n</code></pre> <p>You should see something like this:</p> <pre><code>total 13\n8 lrwxr-xr-x  1 root   admin     1 Jul 28 09:41 Macintosh HD -&gt; /\n5 drwxrwxrwx@ 1 mitch  staff  2560 Jul 28 11:47 boot\n</code></pre> <p>The volume named <code>boot</code> should be the SD card with the Raspbian image on it.</p> </li> <li> <p>Enable SSH: There was a security update to the Raspbian images. Now to enable ssh by default you have to do the following:</p> <pre><code>touch /Volumes/boot/ssh\n</code></pre> <p>This will write an empty file to the root of your Raspbian image. That will enable ssh on startup.</p> </li> <li> <p>Edit <code>config.txt</code></p> <ul> <li>In the root folder of the SD card, open <code>config.txt</code> (<code>/Volumes/boot/config.txt</code>) in a text editor.</li> <li> <p>Append this line to the bottom of it:</p> <pre><code>dtoverlay=dwc2\n</code></pre> </li> <li> <p>Save the file</p> </li> </ul> </li> <li> <p>Edit <code>cmdline.txt</code></p> <ul> <li>In the root folder of the SD card, open <code>cmdline.txt</code> (<code>/Volumes/boot/cmdline.txt</code>) in a text editor</li> <li> <p>After <code>rootwait</code>, append this text leaving only one space between <code>rootwait</code> and the new text (otherwise it might not be parsed correctly):</p> <pre><code>modules-load=dwc2,g_ether\n</code></pre> </li> <li> <p>If there was any text after the new text make sure that there is only one space between that text and the new text.</p> </li> <li>Save the file</li> </ul> </li> <li> <p>Boot the Pi</p> <ul> <li>Give the Pi plenty of time to bootup (can take as much as 90 seconds or more)</li> <li>You can monitor the RNDIS/Ethernet Gadget status in the System Preferences / Network panel (note that the IP address listed is not the host)</li> </ul> </li> <li> <p>Login over USB</p> <ul> <li> <p>If this is not the first time connecting to your raspberry pi, refresh the SSH key by typing the command: </p> <pre><code>ssh-keygen -R &lt;hostname&gt;.local\n</code></pre> <p>If this is the first time connecting to the Pi, you won't need to run this command.</p> </li> <li> <p>Connect to the Pi:</p> <pre><code>ssh &lt;username&gt;@&lt;hostname&gt;.local\n</code></pre> </li> </ul> </li> </ol>"}]}